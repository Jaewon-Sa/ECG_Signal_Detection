{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d52841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.transform import resize\n",
    "from torchvision import transforms\n",
    "import torchaudio.transforms as ta_transforms\n",
    "import math\n",
    "import torchaudio\n",
    "import time\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935d1757",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=os.getenv(\"HOME\")+\"/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data\"\n",
    "SAMPLE_RATE = 4000\n",
    "HOP_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f7463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patient IDs: ['/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/84706.txt', '/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/85262.txt', '/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/84699.txt']\n",
      "Test Patient IDs: ['/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/50826.txt', '/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/50671.txt', '/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/85174.txt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# txt파일 불러오기\n",
    "file_list = os.listdir(PATH)\n",
    "txt_list = [os.path.join(PATH, file) for file in file_list if file.endswith(\".txt\")]\n",
    "\n",
    "# 환자 아이디를 훈련 데이터셋과 테스트 데이터셋으로 나눔\n",
    "train_patient_txt, test_patient_txt = train_test_split(txt_list, test_size=0.9, random_state=42)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Train Patient IDs:\", train_patient_txt[:3])\n",
    "print(\"Test Patient IDs:\", test_patient_txt[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77acb046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Biquad:\n",
    "\n",
    "  # pretend enumeration\n",
    "    LOWPASS, HIGHPASS, BANDPASS, NOTCH, PEAK, LOWSHELF, HIGHSHELF = range(7)\n",
    "\n",
    "    def __init__(self,typ, freq, srate, Q, dbGain = 0):\n",
    "        types = {\n",
    "            Biquad.LOWPASS : self.lowpass,\n",
    "            Biquad.HIGHPASS : self.highpass,\n",
    "            Biquad.BANDPASS : self.bandpass,\n",
    "            Biquad.NOTCH : self.notch,\n",
    "            Biquad.PEAK : self.peak,\n",
    "            Biquad.LOWSHELF : self.lowshelf,\n",
    "            Biquad.HIGHSHELF : self.highshelf\n",
    "          }\n",
    "        assert typ in types\n",
    "        freq = float(freq)\n",
    "        self.srate = float(srate)\n",
    "        Q = float(Q)\n",
    "        dbGain = float(dbGain)\n",
    "        self.a0 = self.a1 = self.a2 = 0\n",
    "        self.b0 = self.b1 = self.b2 = 0\n",
    "        self.x1 = self.x2 = 0\n",
    "        self.y1 = self.y2 = 0\n",
    "        # only used for peaking and shelving filter types\n",
    "        A = math.pow(10, dbGain / 40)\n",
    "        omega = 2 * math.pi * freq / self.srate\n",
    "        sn = math.sin(omega)\n",
    "        cs = math.cos(omega)\n",
    "        alpha = sn / (2*Q)\n",
    "        beta = math.sqrt(A + A)\n",
    "        types[typ](A,omega,sn,cs,alpha,beta)\n",
    "        # prescale constants\n",
    "        self.b0 /= self.a0\n",
    "        self.b1 /= self.a0\n",
    "        self.b2 /= self.a0\n",
    "        self.a1 /= self.a0\n",
    "        self.a2 /= self.a0\n",
    "\n",
    "    def lowpass(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = (1 - cs) /2\n",
    "        self.b1 = 1 - cs\n",
    "        self.b2 = (1 - cs) /2\n",
    "        self.a0 = 1 + alpha\n",
    "        self.a1 = -2 * cs\n",
    "        self.a2 = 1 - alpha\n",
    "\n",
    "    def highpass(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = (1 + cs) /2\n",
    "        self.b1 = -(1 + cs)\n",
    "        self.b2 = (1 + cs) /2\n",
    "        self.a0 = 1 + alpha\n",
    "        self.a1 = -2 * cs\n",
    "        self.a2 = 1 - alpha\n",
    "\n",
    "    def bandpass(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = alpha\n",
    "        self.b1 = 0\n",
    "        self.b2 = -alpha\n",
    "        self.a0 = 1 + alpha\n",
    "        self.a1 = -2 * cs\n",
    "        self.a2 = 1 - alpha\n",
    "\n",
    "    def notch(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = 1\n",
    "        self.b1 = -2 * cs\n",
    "        self.b2 = 1\n",
    "        self.a0 = 1 + alpha\n",
    "        self.a1 = -2 * cs\n",
    "        self.a2 = 1 - alpha\n",
    "\n",
    "    def peak(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = 1 + (alpha * A)\n",
    "        self.b1 = -2 * cs\n",
    "        self.b2 = 1 - (alpha * A)\n",
    "        self.a0 = 1 + (alpha /A)\n",
    "        self.a1 = -2 * cs\n",
    "        self.a2 = 1 - (alpha /A)\n",
    "\n",
    "    def lowshelf(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = A * ((A + 1) - (A - 1) * cs + beta * sn)\n",
    "        self.b1 = 2 * A * ((A - 1) - (A + 1) * cs)\n",
    "        self.b2 = A * ((A + 1) - (A - 1) * cs - beta * sn)\n",
    "        self.a0 = (A + 1) + (A - 1) * cs + beta * sn\n",
    "        self.a1 = -2 * ((A - 1) + (A + 1) * cs)\n",
    "        self.a2 = (A + 1) + (A - 1) * cs - beta * sn\n",
    "\n",
    "    def highshelf(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = A * ((A + 1) + (A - 1) * cs + beta * sn)\n",
    "        self.b1 = -2 * A * ((A - 1) + (A + 1) * cs)\n",
    "        self.b2 = A * ((A + 1) + (A - 1) * cs - beta * sn)\n",
    "        self.a0 = (A + 1) - (A - 1) * cs + beta * sn\n",
    "        self.a1 = 2 * ((A - 1) - (A + 1) * cs)\n",
    "        self.a2 = (A + 1) - (A - 1) * cs - beta * sn\n",
    "\n",
    "  # perform filtering function\n",
    "    def __call__(self,x):\n",
    "        y = self.b0 * x + self.b1 * self.x1 + self.b2 * self.x2 - self.a1 * self.y1 - self.a2 * self.y2\n",
    "        self.x2, self.x1 = self.x1, x\n",
    "        self.y2, self.y1 = self.y1, y\n",
    "        \n",
    "        return y\n",
    "\n",
    "  # provide a static result for a given frequency f\n",
    "    def result(self,f):\n",
    "        phi = (math.sin(math.pi * f * 2/(2.0 * self.srate)))**2\n",
    "        return ((self.b0+self.b1+self.b2)**2 - \\\n",
    "        4*(self.b0*self.b1 + 4*self.b0*self.b2 + \\\n",
    "        self.b1*self.b2)*phi + 16*self.b0*self.b2*phi*phi) / \\\n",
    "        ((1+self.a1+self.a2)**2 - 4*(self.a1 + \\\n",
    "        4*self.a2 + self.a1*self.a2)*phi + 16*self.a2*phi*phi)\n",
    "\n",
    "    def log_result(self,f):\n",
    "        try:\n",
    "            r = 10 * math.log10(self.result(f))\n",
    "        except:\n",
    "            r = -200\n",
    "        return r\n",
    "\n",
    "  # return computed constants\n",
    "    def constants(self):\n",
    "        return self.b0,self.b1,self.b2,self.a1,self.a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb584ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path, txt_list,\n",
    "                 target_size=(40, 2500),\n",
    "                 th=25,\n",
    "                 resizing=False,\n",
    "                 filtering=False,\n",
    "                 filter_hz=500):\n",
    "        self.path = path\n",
    "        self.txt_list = txt_list\n",
    "        self.target_size = target_size\n",
    "        self.th = int(th * SAMPLE_RATE / HOP_LENGTH)\n",
    "        self.resizing = resizing\n",
    "        self.filtering = filtering\n",
    "        self.filter_hz = filter_hz\n",
    "\n",
    "        self.get_file_list()\n",
    "        \n",
    "        self.delete_list=[]\n",
    "        self.x = self.get_mel_spectrogram()\n",
    "        self.y = self.get_label()\n",
    "        self.delete_data()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def get_file_list(self):\n",
    "        self.heas = []\n",
    "        self.wavs = []\n",
    "        self.tsvs = []\n",
    "\n",
    "        for path_txt in self.txt_list:\n",
    "            with open(path_txt, \"r\") as f:\n",
    "                P_id, n, sr = f.readline().split()\n",
    "                for _ in range(int(n)):\n",
    "                    _, hea, wav, tsv = f.readline().split()\n",
    "                    self.heas.append(hea)\n",
    "                    self.wavs.append(wav)\n",
    "                    self.tsvs.append(tsv)\n",
    "        self.heas.sort()\n",
    "        self.wavs.sort()\n",
    "        self.tsvs.sort()\n",
    "\n",
    "    # torchaudio로 필터링 적용\n",
    "    def apply_filter_torchaudio(self, audio, cutoff_hz):\n",
    "        biquad = Biquad(typ=Biquad.LOWPASS, freq=cutoff_hz, srate=SAMPLE_RATE, Q=0.707)\n",
    "        b0, b1, b2, a1, a2 = biquad.constants()\n",
    "        b0 = torch.tensor(b0)\n",
    "        b1 = torch.tensor(b1)\n",
    "        b2 = torch.tensor(b2)\n",
    "        a0 = 1\n",
    "        a1 = torch.tensor(a1)\n",
    "        a2 = torch.tensor(a2)\n",
    "        filtered_audio = torchaudio.functional.biquad(audio, b0, b1, b2, a0, a1, a2)\n",
    "        return filtered_audio\n",
    "\n",
    "    def padding(self, spec, target_length, padding_value=0):\n",
    "        pad_width = target_length - spec.shape[-1]\n",
    "        padded_spec = torch.nn.functional.pad(spec, (0, pad_width, 0, 0), \"constant\", 0)\n",
    "        return padded_spec\n",
    "\n",
    "    def resize_spectrogram(self, spec, new_shape):\n",
    "        resized_spec = transforms.functional.resize(img=spec, size=new_shape, antialias=None)\n",
    "        return resized_spec\n",
    "\n",
    "    # # 나누기 전 이미지에 대한 정규화를 위해 사용\n",
    "    # def normalize_spectrogram(self, wav, spec):\n",
    "    #     wav_max = np.max(np.array(wav))\n",
    "    #     spec_max = np.max(np.array(spec))\n",
    "    #     norm_max = spec_max / wav_max\n",
    "    #     print(norm_max)\n",
    "    #     normalized = spec / norm_max\n",
    "    #     return normalized\n",
    "\n",
    "    # 나누어진 개별 이미지의 최대값을 이용한 정규화\n",
    "    def normalize_spectrogram2(self, spec):\n",
    "        spec_max = np.max(np.array(spec))\n",
    "        spec_min = np.min(np.array(spec))\n",
    "        normalized = (spec-spec_min) / (spec_max-spec_min)\n",
    "        return normalized\n",
    "\n",
    "    # # torchvision을 사용한 정규화\n",
    "    # def normalize_spectrogram3(self, spec):\n",
    "    #     normalized = transforms.Normalize((0), (0.5))(spec)\n",
    "    #     return normalized\n",
    "\n",
    "    def get_mel_spectrogram(self):\n",
    "        audio_list = []\n",
    "        self.scale_list = []\n",
    "        self.iter_list = []\n",
    "\n",
    "        for path_wav in self.wavs:\n",
    "            path = os.path.join(PATH, path_wav)\n",
    "\n",
    "            # Torchaudio 이용하여 파일 로드\n",
    "            x = torchaudio.load(path)[0]\n",
    "            # Filtering\n",
    "            if self.filtering is True:\n",
    "                cutoff_frequency = self.filter_hz\n",
    "                x = self.apply_filter_torchaudio(x, cutoff_frequency)\n",
    "            ms = ta_transforms.MelSpectrogram(sample_rate=SAMPLE_RATE,\n",
    "                                        n_fft=128,\n",
    "                                        win_length=100,\n",
    "                                        n_mels=40,\n",
    "                                        hop_length=HOP_LENGTH)(x)\n",
    "\n",
    "            # 스펙트로그램의 형태를 유지하면서 torchaudio로 불러왔을 때와 같이 0~1로 정규화\n",
    "            # ms = self.normalize_spectrogram(x, ms)\n",
    "\n",
    "            # 원본 wav의 길이가 th보다 길다면 Slicing\n",
    "            if ms.shape[-1] > self.th:\n",
    "                scale = 1\n",
    "                num_splits = ms.shape[-1] // self.th    # wav길이 == th의 배수\n",
    "                if ms.shape[-1] % self.th != 0: # wav길이 != th의 배수\n",
    "                    num_splits += 1\n",
    "                self.iter_list.append(num_splits)\n",
    "\n",
    "                for i in range(num_splits):\n",
    "                    start_idx = i * self.th\n",
    "                    end_idx = (i + 1) * self.th\n",
    "                    split = ms[..., start_idx:end_idx]\n",
    "\n",
    "                    # th보다 길이가 짧다면\n",
    "                    if split.shape[-1] < self.th:\n",
    "                        # Resizing\n",
    "                        if self.resizing is True:\n",
    "                            scale = self.th / split.shape[-1]\n",
    "                            target_shape = (split.shape[-2], self.th)\n",
    "                            split = self.resize_spectrogram(split, target_shape)\n",
    "                        # Padding\n",
    "                        else:\n",
    "                            split = self.padding(split, self.th)\n",
    "                    # 최종 Resizing\n",
    "                    resized = self.resize_spectrogram(split, self.target_size)\n",
    "                    resized = self.normalize_spectrogram2(resized)  # 정규화\n",
    "                    audio_list.append(resized)\n",
    "                    if self.resizing is True:\n",
    "                        self.scale_list.append(scale)\n",
    "\n",
    "            # 원본 wav의 길이가 th보다 짧거나 같다면\n",
    "            else:\n",
    "                self.iter_list.append(1)\n",
    "                scale = 1\n",
    "                # th보다 짧다면\n",
    "                if ms.shape[-1] < self.th:\n",
    "                    # Resizing\n",
    "                    if self.resizing is True:\n",
    "                        scale = self.th / ms.shape[-1]\n",
    "                        target_shape = (ms.shape[-2], self.th)\n",
    "                        ms = self.resize_spectrogram(ms, target_shape)\n",
    "                    # Padding\n",
    "                    else:\n",
    "                        ms = self.padding(ms, self.th)\n",
    "                # 최종 resizing\n",
    "                ms = self.resize_spectrogram(ms, self.target_size)\n",
    "                ms = self.normalize_spectrogram2(ms)    # 정규화\n",
    "                audio_list.append(ms)\n",
    "                if self.resizing is True:\n",
    "                    self.scale_list.append(scale)\n",
    "        return torch.stack(audio_list)\n",
    "\n",
    "    def get_label(self):\n",
    "        labels = []\n",
    "        idx=0\n",
    "        for i, path_tsv in enumerate(self.tsvs):\n",
    "            path = os.path.join(PATH, path_tsv)\n",
    "            tsv_data = pd.read_csv(path, sep='\\t', header=None)\n",
    "            iter = self.iter_list[i]\n",
    "            continuous = False\n",
    "            next_end = 0\n",
    "            next_class = 0\n",
    "            for _iter in range(iter):\n",
    "                label = []\n",
    "                if self.resizing is True:\n",
    "                    scale = self.scale_list[sum(self.iter_list[:i]) + _iter]\n",
    "                for _, tsv_row in tsv_data.iterrows():\n",
    "                    # 이전 구간에서 이어진다면 tsv_row[0] = 0\n",
    "                    if continuous is True:\n",
    "                        tsv_row[0] = 0\n",
    "                        tsv_row[1] = next_end\n",
    "                        tsv_row[2] = next_class\n",
    "                        continuous = False\n",
    "                        # resize를 하였다면 라벨 값도 스케일링\n",
    "                        if self.resizing is True:\n",
    "                            tsv_row[0] *= scale\n",
    "                            tsv_row[1] *= scale\n",
    "\n",
    "                    # 이전 구간에서 이어지지 않는다면 새로 데이터 가져오기\n",
    "                    elif tsv_row[2] in [1, 3]:\n",
    "                        # 구간 불러와서 sr값 곱하고 hop_legth로 나누기\n",
    "                        tsv_row[0] = tsv_row[0] * SAMPLE_RATE / HOP_LENGTH - (_iter * self.th)\n",
    "                        tsv_row[1] = tsv_row[1] * SAMPLE_RATE / HOP_LENGTH - (_iter * self.th)\n",
    "                        tsv_row[2] = 0 if tsv_row[2] == 1 else 1    # S1=0, S2=1 \n",
    "                        # 시작점이 th 이상이라면, 이전 구간 이미지의 라벨이라면 continue\n",
    "                        if tsv_row[0] >= self.th or tsv_row[0] < 0:\n",
    "                            continue\n",
    "                        # th보다 길이가 짧다면(나뉜 이미지의 가장 마지막 이미지라면)\n",
    "                        if _iter == iter - 1:\n",
    "                            # resize를 하였다면 라벨 값도 스케일링\n",
    "                            if self.resizing is True:\n",
    "                                tsv_row[0] *= scale\n",
    "                                tsv_row[1] *= scale\n",
    "\n",
    "                        # th보다 길이가 길다면 Slicing\n",
    "                        elif tsv_row[1] >= self.th:\n",
    "                            next_end = tsv_row[1] - self.th\n",
    "                            next_class = tsv_row[2]\n",
    "                            tsv_row[1] = self.th - 1\n",
    "                            continuous = True\n",
    "                            # 최종 resize한 값 으로 보간\n",
    "                            tsv_row[0] *= self.target_size[1] / self.th\n",
    "                            tsv_row[1] *= self.target_size[1] / self.th\n",
    "                            \n",
    "                            # S1=1, S2=2 \n",
    "                            label.append([tsv_row[0] / self.target_size[0], 0 / self.target_size[0],\n",
    "                                  tsv_row[1] / self.target_size[0], self.target_size[0] / self.target_size[0],\n",
    "                                  int(tsv_row[2])+1])# xmin, ymin, xmax, ymax, cls\n",
    "                            break\n",
    "\n",
    "                    # 이전 값에서 이어지지 않으면서 S1, S2가 아닌 경우 continue\n",
    "                    else: continue\n",
    "\n",
    "                    # 최종 resize한 값 으로 보간\n",
    "                    tsv_row[0] *= self.target_size[1] / self.th\n",
    "                    tsv_row[1] *= self.target_size[1] / self.th\n",
    "                    \n",
    "                    label.append([tsv_row[0] / self.target_size[0], 0 / self.target_size[0],\n",
    "                                  tsv_row[1] / self.target_size[0], self.target_size[0] / self.target_size[0],\n",
    "                                  int(tsv_row[2])+1])# xmin, ymin, xmax, ymax, cls\n",
    "                    \n",
    "                    #label.append((int(tsv_row[2]), tsv_row[0], tsv_row[1]))\n",
    "                if(len(label)==0):\n",
    "                    self.delete_list.append(idx)\n",
    "                idx+=1\n",
    "                labels.append(label)      \n",
    "        return labels\n",
    "    \n",
    "    def delete_data(self):\n",
    "        delete_count=0\n",
    "        for i in self.delete_list:\n",
    "            del self.y[i-delete_count]\n",
    "            delete_count+=1\n",
    "        \n",
    "        self.x = self.x[[i for i in range(self.x.size(0)) if i not in self.delete_list]]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad486d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(PATH, train_patient_txt, target_size=(300, 300), th=5, resizing=True, filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb0a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049 1049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.delete_list)\n",
    "dataset.delete_list\n",
    "print(len(dataset.y),len(dataset.x))\n",
    "dataset.x.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f105ade5",
   "metadata": {},
   "source": [
    "# data loader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c1a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])  # sample[0]은 화상 gt\n",
    "        targets.append(torch.FloatTensor(sample[1]))  # sample[1]은 어노테이션 gt\n",
    "\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    return imgs, targets\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e38d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "torch.Size([32, 1, 300, 300])\n",
      "torch.Size([20, 5])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "for images, labels in train_dataloader:\n",
    "    print(images.size())\n",
    "    print(labels[0].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2e34d",
   "metadata": {},
   "source": [
    "# model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a0c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "backbone = models.mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "#for param in backbone.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "947b2c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "avgpool\n",
      "classifier\n",
      "ConvBNActivation(\n",
      "  (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for name, module in backbone.named_children():\n",
    "    print(name)\n",
    "\n",
    "for n, x in enumerate(backbone.features.children()):\n",
    "    if n==0:\n",
    "        seq=x.children()\n",
    "        seq=next(seq)\n",
    "        \n",
    "        prev_weight = seq.weight\n",
    "        new_weight = prev_weight[:, :1, :, :]\n",
    "        seq.weight = nn.Parameter(new_weight)\n",
    "        seq.in_channels = 1\n",
    "    if n==13:\n",
    "        seq=x.children()\n",
    "        seq=next(seq)\n",
    "        print(seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a0e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class SSD(nn.Module):\n",
    "    def __init__(self, backbone, n_class=3, default_box_n=[4,6,6,6,4,4], state = \"Train\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        \n",
    "        self.n_class = n_class\n",
    "        self.default_box_n = default_box_n\n",
    "        self.state = state\n",
    "        if self.state != \"Train\":\n",
    "            self.softmax = nn.Softmax(dim=-1)\n",
    "            \n",
    "        #가중치 초기화 인자\n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 112, 1, 1))  # there are 512 channels in conv4_3_feats\n",
    "        nn.init.constant_(self.rescale_factors, 20)\n",
    "        \n",
    "        #backbone\n",
    "        \n",
    "        self.backbone_layer = nn.Sequential(\n",
    "            backbone.features,\n",
    "        )\n",
    "        \n",
    "        #extra layer\n",
    "        self.extra_layer_1 = self.extra_layers(960,512,4)\n",
    "        self.extra_layer_2 = self.extra_layers(512,256,4)\n",
    "        self.extra_layer_3 = self.extra_layers(256,256,2)\n",
    "        self.extra_layer_4 = self.extra_layers(256,128,2)\n",
    "        \n",
    "        self.extra_layers = [self.extra_layer_1, \n",
    "                            self.extra_layer_2, \n",
    "                            self.extra_layer_3, \n",
    "                            self.extra_layer_4]\n",
    "        \n",
    "        \n",
    "        #detection output\n",
    "        \n",
    "        self.cls_layers = nn.ModuleList([\n",
    "                                        nn.Conv2d(672, default_box_n[0]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(960, default_box_n[1]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(512, default_box_n[2]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(256, default_box_n[3]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(256, default_box_n[4]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(128, default_box_n[5]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                                        ])\n",
    "        \n",
    "        self.loc_layers = nn.ModuleList([\n",
    "                                        nn.Conv2d(672, default_box_n[0]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(960, default_box_n[1]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(512, default_box_n[2]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(256, default_box_n[3]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(256, default_box_n[4]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(128, default_box_n[5]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                                        ])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        #가중치 초기화 함수\n",
    "        \n",
    "        #모델학습이 순조롭지않다면 향후 추가 예정\n",
    "        return\n",
    "    \n",
    "    def extra_layers(self, input_size, output_size, div):\n",
    "        layer = nn.Sequential(\n",
    "            #conv2D 해상도낮추기\n",
    "            nn.Conv2d(input_size, output_size, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(output_size, eps=0.001, momentum=0.01, affine=True, track_running_stats=True),\n",
    "            nn.Hardswish(),\n",
    "            \n",
    "            #Inverted Residual (mobilev2 + squeeze)\n",
    "            \n",
    "            #depthwise \n",
    "            # kernel size 5 고려해보기\n",
    "            nn.Conv2d(output_size, output_size, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=output_size, bias=False),\n",
    "            nn.BatchNorm2d(output_size, eps=0.001, momentum=0.01, affine=True, track_running_stats=True),\n",
    "            nn.Hardswish(),\n",
    "            \n",
    "            #SqueezeExcitation\n",
    "            nn.Conv2d(output_size, output_size//div, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_size//div, output_size, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            \n",
    "            #Point-wise\n",
    "            nn.Conv2d(output_size, output_size, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.Hardswish(),\n",
    "            nn.Identity()\n",
    "            \n",
    "        ) \n",
    "        return layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        f_maps=[]\n",
    "        for n, layer in enumerate(backbone.features):#12 16\n",
    "            if n==13: \n",
    "                #L2 norm\n",
    "                #norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()     # (N, 1, 19, 19)\n",
    "                #featureMap_1 = x / norm                                  # (N, 112, 19, 19)\n",
    "                #featureMap_1= conv4_3 * self.rescale_factors            # (N, 512, 19, 19)\n",
    "                \n",
    "                #13계층 bottleneck까지만\n",
    "                seq_layer = next(layer.children())\n",
    "                for seq_n in range(4):\n",
    "                    x = seq_layer[seq_n](x)\n",
    "                    if seq_n==0:\n",
    "                        f_maps.append(x)\n",
    "                continue\n",
    "                \n",
    "            x = layer(x)\n",
    "            #print(\"size : {0}, number = {1}\".format(x.size(), n))\n",
    "            if n==16:\n",
    "                f_maps.append(x) \n",
    "                \n",
    "        for extra_layer in self.extra_layers:\n",
    "            x = extra_layer(x)\n",
    "            \n",
    "            f_maps.append(x)\n",
    "            \n",
    "        cls = []\n",
    "        loc = []\n",
    "        for f_map, cls_layer, loc_layer in zip(f_maps,self. cls_layers, self.loc_layers):\n",
    "            output_cls = cls_layer(f_map)\n",
    "            output_loc = loc_layer(f_map)\n",
    "            \n",
    "            cls.append(output_cls.permute(0, 2, 3, 1).contiguous())\n",
    "            loc.append(output_loc.permute(0, 2, 3, 1).contiguous())\n",
    "            #cls.append(output_cls)\n",
    "            #loc.append(output_loc)\n",
    "            \n",
    "        cls = torch.cat([o.view(o.size(0), -1) for o in cls], 1)\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "\n",
    "        loc = loc.view(loc.size(0), -1, 4)\n",
    "        if self.state == \"Train\":\n",
    "            cls = cls.view(cls.size(0), -1, self.n_class)\n",
    "        else:\n",
    "            cls = self.softmax(cls.view(cls.size(0), -1, self.n_class))\n",
    "            \n",
    "            \n",
    "        return cls, loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043b596",
   "metadata": {},
   "source": [
    "# default box, loss 함수 기타 기능 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fcde8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "def point_form(boxes):\n",
    "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n",
    "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n",
    "\n",
    "\n",
    "def center_size(boxes):\n",
    "    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n",
    "                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n",
    "\n",
    "\n",
    "def intersect(box_a, box_b):\n",
    "    A = box_a.size(0)\n",
    "    B = box_b.size(0)\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "\n",
    "def jaccard(box_a, box_b):# iou\n",
    "    inter = intersect(box_a, box_b)\n",
    "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
    "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
    "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
    "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]\n",
    "\n",
    "\n",
    "def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n",
    "    # jaccard index\n",
    "    # defuatl box와 truths box iou 계산\n",
    "    overlaps = jaccard(\n",
    "        truths,\n",
    "        point_form(priors)\n",
    "    )\n",
    "    # (Bipartite Matching)\n",
    "    # [1,num_objects] best prior for each ground truth\n",
    "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n",
    "    # [1,num_priors] best ground truth for each prior\n",
    "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
    "    best_truth_idx.squeeze_(0)\n",
    "    best_truth_overlap.squeeze_(0)\n",
    "    best_prior_idx.squeeze_(1)\n",
    "    best_prior_overlap.squeeze_(1)\n",
    "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n",
    "    # TODO refactor: index  best_prior_idx with long tensor\n",
    "    # ensure every gt matches with its prior of max overlap\n",
    "    for j in range(best_prior_idx.size(0)):\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n",
    "    conf = labels[best_truth_idx]#+1         # Shape: [num_priors]\n",
    "    conf[best_truth_overlap < threshold] = 0  # label as background\n",
    "    loc = encode(matches, priors, variances)\n",
    "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
    "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
    "    \n",
    "\n",
    "\n",
    "def encode(matched, priors, variances):\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = torch.log(g_wh) / variances[1]\n",
    "    # return target for smooth_l1_loss\n",
    "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n",
    "\n",
    "\n",
    "# Adapted from https://github.com/Hakuyume/chainer-ssd\n",
    "def decode(loc, priors, variances):\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    x_max = x.data.max()\n",
    "    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n",
    "\n",
    "\n",
    "def nms(boxes, scores, overlap=0.5, top_k=200):\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0:\n",
    "        return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort in ascending order\n",
    "    # I = I[v >= 0.01]\n",
    "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    # keep = torch.Tensor()\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        # keep.append(i)\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1:\n",
    "            break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= overlap\n",
    "        idx = idx[IoU.le(overlap)]\n",
    "    return keep, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a263f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"SSD의 손실함수 클래스 \"\"\"\n",
    "\n",
    "    def __init__(self, thresh=0.5, neg_pos=3, device='cpu'):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.jaccard_thresh = thresh  # 0.5 match 함수의 jaccard 계수의 임계치\n",
    "        self.negpos_ratio = neg_pos  # 3:1 Hard Negative Mining의 음과 양 비율\n",
    "        self.device = device  # 계산 device(CPU | GPU)\n",
    "\n",
    "    def forward(self, predictions, targets, dboxs):\n",
    "        \"\"\"\n",
    "        파라미터 설명\n",
    "        ----------\n",
    "        predictions :모델의 예측값 cls와 loc\n",
    "        cls는 batch dbox의 개수, 클래스 개수로 이루어짐\n",
    "        loc은 batch dbox의 개수, 4\n",
    "        \n",
    "        targets : [num_batch, 객체개수, 5]\n",
    "            5는 라벨 정보[xmin, ymin, xmax, ymax, label_ind]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        conf_data, loc_data = predictions\n",
    "        dbox_list = dboxs\n",
    "\n",
    "        num_batch = loc_data.size(0)  # 배치 크기\n",
    "        num_dbox = loc_data.size(1)  # DBox의 수 \n",
    "        num_classes = conf_data.size(2)  # 클래스 수\n",
    "\n",
    "        # 손실 계산에 사용할 것을 저장하는 변수 작성\n",
    "        # conf_t_label：각 DBox에 가장 가까운 정답 BBox의 라벨을 저장 \n",
    "        # loc_t: 각 DBox에 가장 가까운 정답 BBox의 위치 정보 저장 \n",
    "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
    "        \n",
    "        #conf_t_label.fill_(0)#테스트용도\n",
    "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
    "\n",
    "        # loc_t와 conf_t_label에 \n",
    "        # DBox와 정답 어노테이션 targets를 amtch한 결과 덮어쓰기\n",
    "        for idx in range(num_batch):  # 미니 배치 루프\n",
    "\n",
    "            truths = targets[idx][:, :-1].to(self.device)\n",
    "            labels = targets[idx][:, -1].to(self.device)\n",
    "            dbox = dbox_list.to(self.device)\n",
    "\n",
    "            # match 함수를 실행하여 loc_t와 conf_t_label 내용 갱신\n",
    "            # loc_t: 각 DBox에 가장 가까운 정답 BBox 위치 정보가 덮어써짐.\n",
    "            # conf_t_label：각 DBox에 가장 가까운 정답 BBox 라벨이 덮어써짐.\n",
    "            # 단, 가장 가까운 BBox와 iou가 0.5보다 작은 경우,\n",
    "            # 정답 BBox의 라벨 conf_t_label은 배경 클래스 0으로 한다.\n",
    "            variance = [0.1, 0.2]\n",
    "            \n",
    "            # 라벨을 dbox에 대한 offset으로 변환\n",
    "            match(self.jaccard_thresh, truths, dbox,\n",
    "                  variance, labels, loc_t, conf_t_label, idx)\n",
    "\n",
    "        #물체를 발견한 offset만 손실 계산\n",
    "        pos_mask = conf_t_label > 0  \n",
    "\n",
    "        # pos_mask를 loc_data 크기로 변형\n",
    "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
    "\n",
    "        # Positive DBox의 loc_data와 offset loc_t 취득\n",
    "        #print(loc_data.size())\n",
    "        #print(pos_mask.size())\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4).to(device)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "\n",
    "        # 물체를 발견한 Positive DBox의 오프셋 정보 loc_t의 손실(오차)를 계산\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
    "\n",
    "        # ----------\n",
    "        # 클래스 예측의 손실 : loss_c를 계산\n",
    "        # 교차 엔트로피 오차 함수로 손실 계산. 단 배경 클래스가 정답인 DBox가 압도적으로 많으므로,\n",
    "        # Hard Negative Mining을 실시하여 물체 발견 DBox 및 배경 클래스 DBox의 비율이 1:3이 되도록 한다.\n",
    "        # 배경 클래스 DBox로 예상한 것 중 손실이 적은 것은 클래스 예측 손실에서 제외\n",
    "        # ----------\n",
    "        batch_conf = conf_data.view(-1, num_classes)\n",
    "\n",
    "        # 클래스 예측의 손실함수 계산(reduction='none'으로 하여 합을 취하지 않고 차원 보존)\n",
    "        loss_c = F.cross_entropy(\n",
    "            batch_conf, conf_t_label.view(-1), reduction='none')\n",
    "        \n",
    "        \n",
    "        #------------이 아래 부분은 이해 못함 \n",
    "\n",
    "        # -----------------\n",
    "        # Negative DBox중 Hard Negative Mining으로 \n",
    "        # 추출하는 것을 구하는 마스크 작성\n",
    "        # -----------------\n",
    "\n",
    "        # 물체를 발견한 Positive DBox의 손실을 0으로 한다.\n",
    "        # (주의) 물체는 label이 1 이상, 라벨 0은 배경을 의미\n",
    "        num_pos = pos_mask.long().sum(1, keepdim=True)  # 미니 배치별 물체 클래스 예측 수\n",
    "        loss_c = loss_c.view(num_batch, -1)  # torch.Size([num_batch, 8732])\n",
    "        loss_c[pos_mask] = 0  # 물체를 발견한 DBox는 손실 0으로 한다.\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        # 각 DBox 손실의 크기 loss_c 순위 idx_rank를 구함\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "\n",
    "        #  (주의) 구현된 코드는 특수하며 직관적이지 않음.\n",
    "        # 위 두 줄의 요점은 각 DBox에 대해 손실 크기가 몇 번째인지의 정보를 \n",
    "        # idx_rank 변수로 빠르게 얻는 코드이다.\n",
    "        \n",
    "        # DBox의 손실 값이 큰 쪽부터 내림차순으로 정렬하여, \n",
    "        # DBox의 내림차순의 index를 loss_idx에 저장한다.\n",
    "        # 손실 크기 loss_c의 순위 idx_rank를 구한다.\n",
    "        # 내림차순이 된 배열 index인 loss_idx를 0부터 8732까지 오름차순으로 다시 정렬하기 위하여\n",
    "        # 몇 번째 loss_idx의 인덱스를 취할 지 나타내는 것이 idx_rank이다.\n",
    "        # 예를 들면 idx_rank 요소의 0번째 = idx_rank[0]을 구하는 것은 loss_idx의 값이 0인 요소,\n",
    "        # 즉 loss_idx[?] =0은 원래 loss_c의 요소 0번째는 내림차순으로 정렬된 loss_idx의 \n",
    "        # 몇 번째입니까? 를구하는 것이 되어 결과적으로, \n",
    "        # ? = idx_rank[0]은 loss_c의 요소 0번째가 내림차순으로 몇 번째인지 나타냄\n",
    "\n",
    "        # 배경 DBox의 수 num_neg를 구한다. HardNegative Mining으로 \n",
    "        # 물체 발견 DBox으 ㅣ수 num_pos의 세 배 (self.negpos_ratio 배)로 한다.\n",
    "        # DBox의 수를 초과한 경우에는 DBox의 수를 상한으로 한다.\n",
    "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\n",
    "\n",
    "        # idx_rank에 각 DBox의 손실 크기가 위에서부터 몇 번째인지 저장되었다.\n",
    "        # 배경 DBox의 수 num_neg보다 순위가 낮은(손실이 큰) DBox를 취하는 마스크 작성\n",
    "        # torch.Size([num_batch, 8732])\n",
    "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
    "\n",
    "        # -----------------\n",
    "        # (종료) 지금부터 Negative DBox 중 Hard Negative Mining으로 추출할 것을 구하는 마스크를 작성\n",
    "        # -----------------\n",
    "\n",
    "        # 마스크 모양을 고쳐 conf_data에 맞춘다\n",
    "        # pos_idx_mask는 Positive DBox의 conf를 꺼내는 마스크이다.\n",
    "        # neg_idx_mask는 Hard Negative Mining으로 추출한 Negative DBox의 conf를 꺼내는 마스크이다.\n",
    "        # pos_mask：torch.Size([num_batch, 8732])\n",
    "        # --> pos_idx_mask：torch.Size([num_batch, 8732, 21])\n",
    "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
    "\n",
    "        # conf_data에서 pos와 neg만 꺼내서 conf_hnm으로 한다. \n",
    "        # 형태는 torch.Size([num_pos+num_neg, 21])\n",
    "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)\n",
    "                             ].view(-1, num_classes)\n",
    "        # gt는 greater than (>)의 약칭. mask가 1인 index를 꺼낸다\n",
    "        # pos_idx_mask+neg_idx_mask는 덧셈이지만 index로 mask를 정리할 뿐임.\n",
    "        # pos이든 neg이든 마스크가 1인 것을 더해 하나의 리스트로 만들어 이를 gt로 췯그한다.\n",
    "\n",
    "        # 마찬가지로 지도 데이터인 conf_t_label에서 pos와 neg만 꺼내, conf_t_label_hnm 으로 \n",
    "        # torch.Size([pos+neg]) 형태가 된다\n",
    "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
    "\n",
    "        # confidence의 손실함수 계산(요소의 합계=sum을 구함)\n",
    "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\n",
    "\n",
    "        # 물체를 발견한 BBox의 수 N (전체 미니 배치의 합계) 으로 손실을 나눈다.\n",
    "        N = num_pos.sum()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "\n",
    "        return loss_l, loss_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c681ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt\n",
    "from itertools import product as product\n",
    "\n",
    "class default:\n",
    "    \n",
    "    def __init__(self, image_size=300, feature_maps=[19, 10, 5, 3, 2, 1], min_sizes=[]):\n",
    "        super(default, self).__init__()\n",
    "        self.image_size = 300\n",
    "        # number of priors for feature map location (either 4 or 6)\n",
    "        self.feature_maps = feature_maps\n",
    "        self.min_sizes = [16, 30, 60, 100, 150, 300]  #0.2, 0.34, 0.48, 0.62, 0.76, 0.9?\n",
    "        self.max_sizes = [30, 60, 100, 150, 300, 300]\n",
    "        self.steps = [19,10,5,3,2,1] # 이미지 그리드로 나눈 개수\n",
    "        self.aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
    "        self.clip = True\n",
    "\n",
    "    def forward(self):\n",
    "        mean = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):\n",
    "                f_k = self.steps[k] # 그리드 개수\n",
    "                # default 박스 중점\n",
    "                cx = (j + 0.5) / f_k\n",
    "                cy = (i + 0.5) / f_k\n",
    "                # aspect_ratio: 1\n",
    "                # default 박스 공식이 아닌 임의로 설정 그리드 크기를 기준으로 나눔\n",
    "                s_k = self.min_sizes[k]/self.image_size\n",
    "                mean += [cx, cy, s_k, 1] # s_k\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # s_k*s_k+1\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
    "                \n",
    "                #print(cx,cy,s_k_prime,self.max_sizes[k]/self.image_size)\n",
    "                mean += [cx, cy, s_k_prime, 1] #s_k\n",
    "\n",
    "                # rest of aspect ratios\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    mean += [cx, cy, s_k*sqrt(ar), 1] # s_k/sqrt(ar)\n",
    "                    mean += [cx, cy, s_k/sqrt(ar), 1]\n",
    "        # back to torch land\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output\n",
    "    def cxcy_to_xy(self, tensor_default_box):\n",
    "        return tensor_default_box*self.image_size\n",
    "    \n",
    "    def xy_to_cxcy(self, tensor_default_box):\n",
    "        return tensor_default_box/self.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04cec5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def train_step(model, Data_loader, epoch_num, lr=0.0000001):\n",
    "    d=default()\n",
    "    tensor_d = d.forward()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_func = MultiBoxLoss(device=device)\n",
    "    \n",
    "    epoch_train_loss = 0.0 # 에포크 손실 합\n",
    "    #epoch_val_loss = 0.0\n",
    "    \n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch : {0} / {1}\".format(epoch+1,epoch_num))\n",
    "        \n",
    "        #targets=np.array([[[1,2,3,4,5],[1,2,3,4,5]],[[1,2,3,4,5],[1,2,3,4,5]]])\n",
    "        for idx, data in enumerate(Data_loader):\n",
    "            iter_start = time.time()\n",
    "            #img=torch.zeros((32,3,300,300),dtype=torch.float)\n",
    "\n",
    "            images = data[0].to(device)\n",
    "\n",
    "            labels = [label.to(device) for label in data[1]]\n",
    "            optimizer.zero_grad()#이전 값 들에 대한 가중치 기울기 초기화\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                cls, loc = model(images)\n",
    "                tensor_d.to(device)\n",
    "                loss_l, loss_c = loss_func((cls, loc), labels, tensor_d)\n",
    "                \n",
    "                loss = loss_l + loss_c\n",
    "                loss.backward()\n",
    "                optimizer.step() # 파라미터 갱신\n",
    "            if(idx % 10 == 0):\n",
    "                iter_end = time.time()\n",
    "                print(\"Current Batch {0} / {1} | Cls Loss : {2:.3f}, Loc Loss : {3:.3f}, Total Loss : {4:.3f} | 10 iter time {5:.4f}: \"\n",
    "                      .format(idx, len(Data_loader), loss_l.item(), loss_c.item(), loss.item(), iter_end - iter_start))\n",
    "\n",
    "                \n",
    "                iter_start =time.time()\n",
    "                \n",
    "            epoch_train_loss+=loss.item()\n",
    "                \n",
    "        print(\"Epoch : {0} / {1} of Total Loss : Total Loss : {2:.3f}\".format(epoch+1, epoch_num, epoch_train_loss))\n",
    "        print(\"-----------------------------------------------\")\n",
    "        epoch_train_loss=0\n",
    "        \n",
    "        torch.save(model, \n",
    "                './objectdetection_model/ssd300_' + str(epoch+1) + '.pth')\n",
    "        torch.save(model.state_dict(), \n",
    "                './objectdetection_model/ssd300_weight_' + str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73acb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SSD(backbone, n_class=3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1fa8b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 / 10\n",
      "Current Batch 0 / 33 | Cls Loss : nan, Loc Loss : 7.699, Total Loss : nan | 10 iter time 0.4890: \n",
      "Current Batch 3 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3060: \n",
      "Current Batch 6 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3059: \n",
      "Current Batch 9 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3068: \n",
      "Current Batch 12 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3063: \n",
      "Current Batch 15 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3062: \n",
      "Current Batch 18 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3070: \n",
      "Current Batch 21 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3175: \n",
      "Current Batch 24 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3053: \n",
      "Current Batch 27 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3086: \n",
      "Current Batch 30 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3055: \n",
      "Epoch : 1 / 10 of Total Loss : Total Loss : nan\n",
      "-----------------------------------------------\n",
      "Epoch : 2 / 10\n",
      "Current Batch 0 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3043: \n",
      "Current Batch 3 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3070: \n",
      "Current Batch 6 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3152: \n",
      "Current Batch 9 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3041: \n",
      "Current Batch 12 / 33 | Cls Loss : nan, Loc Loss : nan, Total Loss : nan | 10 iter time 0.3071: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_462/682840891.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_462/2822692156.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, Data_loader, epoch_num, lr)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 파라미터 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_step(model, train_dataloader, epoch_num = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f300d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(train_dataloader):\n",
    "    print(idx)\n",
    "    print(data[1][0][:,-1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aadba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18825c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
