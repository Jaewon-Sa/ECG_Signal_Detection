{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3695f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.transform import resize\n",
    "from torchvision import transforms\n",
    "import torchaudio.transforms as ta_transforms\n",
    "import math\n",
    "import torchaudio\n",
    "import time\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20378c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=os.getenv(\"HOME\")+\"/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data\"\n",
    "SAMPLE_RATE = 4000\n",
    "HOP_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83280298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patient IDs: ['/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/84706.txt', '/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/85262.txt', '/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/84699.txt']\n",
      "Test Patient IDs: ['/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/50826.txt', '/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/50671.txt', '/aiffel/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data/85174.txt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# txt파일 불러오기\n",
    "file_list = os.listdir(PATH)\n",
    "txt_list = [os.path.join(PATH, file) for file in file_list if file.endswith(\".txt\")]\n",
    "\n",
    "# 환자 아이디를 훈련 데이터셋과 테스트 데이터셋으로 나눔\n",
    "train_patient_txt, test_patient_txt = train_test_split(txt_list, test_size=0.9, random_state=42)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Train Patient IDs:\", train_patient_txt[:3])\n",
    "print(\"Test Patient IDs:\", test_patient_txt[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e216f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Biquad:\n",
    "\n",
    "  # pretend enumeration\n",
    "    LOWPASS, HIGHPASS, BANDPASS, NOTCH, PEAK, LOWSHELF, HIGHSHELF = range(7)\n",
    "\n",
    "    def __init__(self,typ, freq, srate, Q, dbGain = 0):\n",
    "        types = {\n",
    "            Biquad.LOWPASS : self.lowpass,\n",
    "            Biquad.HIGHPASS : self.highpass,\n",
    "            Biquad.BANDPASS : self.bandpass,\n",
    "            Biquad.NOTCH : self.notch,\n",
    "            Biquad.PEAK : self.peak,\n",
    "            Biquad.LOWSHELF : self.lowshelf,\n",
    "            Biquad.HIGHSHELF : self.highshelf\n",
    "          }\n",
    "        assert typ in types\n",
    "        freq = float(freq)\n",
    "        self.srate = float(srate)\n",
    "        Q = float(Q)\n",
    "        dbGain = float(dbGain)\n",
    "        self.a0 = self.a1 = self.a2 = 0\n",
    "        self.b0 = self.b1 = self.b2 = 0\n",
    "        self.x1 = self.x2 = 0\n",
    "        self.y1 = self.y2 = 0\n",
    "        # only used for peaking and shelving filter types\n",
    "        A = math.pow(10, dbGain / 40)\n",
    "        omega = 2 * math.pi * freq / self.srate\n",
    "        sn = math.sin(omega)\n",
    "        cs = math.cos(omega)\n",
    "        alpha = sn / (2*Q)\n",
    "        beta = math.sqrt(A + A)\n",
    "        types[typ](A,omega,sn,cs,alpha,beta)\n",
    "        # prescale constants\n",
    "        self.b0 /= self.a0\n",
    "        self.b1 /= self.a0\n",
    "        self.b2 /= self.a0\n",
    "        self.a1 /= self.a0\n",
    "        self.a2 /= self.a0\n",
    "\n",
    "    def lowpass(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = (1 - cs) /2\n",
    "        self.b1 = 1 - cs\n",
    "        self.b2 = (1 - cs) /2\n",
    "        self.a0 = 1 + alpha\n",
    "        self.a1 = -2 * cs\n",
    "        self.a2 = 1 - alpha\n",
    "\n",
    "    def highpass(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = (1 + cs) /2\n",
    "        self.b1 = -(1 + cs)\n",
    "        self.b2 = (1 + cs) /2\n",
    "        self.a0 = 1 + alpha\n",
    "        self.a1 = -2 * cs\n",
    "        self.a2 = 1 - alpha\n",
    "\n",
    "    def bandpass(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = alpha\n",
    "        self.b1 = 0\n",
    "        self.b2 = -alpha\n",
    "        self.a0 = 1 + alpha\n",
    "        self.a1 = -2 * cs\n",
    "        self.a2 = 1 - alpha\n",
    "\n",
    "    def notch(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = 1\n",
    "        self.b1 = -2 * cs\n",
    "        self.b2 = 1\n",
    "        self.a0 = 1 + alpha\n",
    "        self.a1 = -2 * cs\n",
    "        self.a2 = 1 - alpha\n",
    "\n",
    "    def peak(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = 1 + (alpha * A)\n",
    "        self.b1 = -2 * cs\n",
    "        self.b2 = 1 - (alpha * A)\n",
    "        self.a0 = 1 + (alpha /A)\n",
    "        self.a1 = -2 * cs\n",
    "        self.a2 = 1 - (alpha /A)\n",
    "\n",
    "    def lowshelf(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = A * ((A + 1) - (A - 1) * cs + beta * sn)\n",
    "        self.b1 = 2 * A * ((A - 1) - (A + 1) * cs)\n",
    "        self.b2 = A * ((A + 1) - (A - 1) * cs - beta * sn)\n",
    "        self.a0 = (A + 1) + (A - 1) * cs + beta * sn\n",
    "        self.a1 = -2 * ((A - 1) + (A + 1) * cs)\n",
    "        self.a2 = (A + 1) + (A - 1) * cs - beta * sn\n",
    "\n",
    "    def highshelf(self,A,omega,sn,cs,alpha,beta):\n",
    "        self.b0 = A * ((A + 1) + (A - 1) * cs + beta * sn)\n",
    "        self.b1 = -2 * A * ((A - 1) + (A + 1) * cs)\n",
    "        self.b2 = A * ((A + 1) + (A - 1) * cs - beta * sn)\n",
    "        self.a0 = (A + 1) - (A - 1) * cs + beta * sn\n",
    "        self.a1 = 2 * ((A - 1) - (A + 1) * cs)\n",
    "        self.a2 = (A + 1) - (A - 1) * cs - beta * sn\n",
    "\n",
    "  # perform filtering function\n",
    "    def __call__(self,x):\n",
    "        y = self.b0 * x + self.b1 * self.x1 + self.b2 * self.x2 - self.a1 * self.y1 - self.a2 * self.y2\n",
    "        self.x2, self.x1 = self.x1, x\n",
    "        self.y2, self.y1 = self.y1, y\n",
    "        \n",
    "        return y\n",
    "\n",
    "  # provide a static result for a given frequency f\n",
    "    def result(self,f):\n",
    "        phi = (math.sin(math.pi * f * 2/(2.0 * self.srate)))**2\n",
    "        return ((self.b0+self.b1+self.b2)**2 - \\\n",
    "        4*(self.b0*self.b1 + 4*self.b0*self.b2 + \\\n",
    "        self.b1*self.b2)*phi + 16*self.b0*self.b2*phi*phi) / \\\n",
    "        ((1+self.a1+self.a2)**2 - 4*(self.a1 + \\\n",
    "        4*self.a2 + self.a1*self.a2)*phi + 16*self.a2*phi*phi)\n",
    "\n",
    "    def log_result(self,f):\n",
    "        try:\n",
    "            r = 10 * math.log10(self.result(f))\n",
    "        except:\n",
    "            r = -200\n",
    "        return r\n",
    "\n",
    "  # return computed constants\n",
    "    def constants(self):\n",
    "        return self.b0,self.b1,self.b2,self.a1,self.a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d651529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path, txt_list,\n",
    "                 target_size=(40, 2500),\n",
    "                 th=25,\n",
    "                 resizing=False,\n",
    "                 filtering=False,\n",
    "                 filter_hz=500):\n",
    "        self.path = path\n",
    "        self.txt_list = txt_list\n",
    "        self.target_size = target_size\n",
    "        self.th = int(th * SAMPLE_RATE / HOP_LENGTH)\n",
    "        self.resizing = resizing\n",
    "        self.filtering = filtering\n",
    "        self.filter_hz = filter_hz\n",
    "\n",
    "        self.get_file_list()\n",
    "        \n",
    "        self.delete_list=[]\n",
    "        self.x = self.get_mel_spectrogram()\n",
    "        self.y = self.get_label()\n",
    "        self.delete_data()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def get_file_list(self):\n",
    "        self.heas = []\n",
    "        self.wavs = []\n",
    "        self.tsvs = []\n",
    "\n",
    "        for path_txt in self.txt_list:\n",
    "            with open(path_txt, \"r\") as f:\n",
    "                P_id, n, sr = f.readline().split()\n",
    "                for _ in range(int(n)):\n",
    "                    _, hea, wav, tsv = f.readline().split()\n",
    "                    self.heas.append(hea)\n",
    "                    self.wavs.append(wav)\n",
    "                    self.tsvs.append(tsv)\n",
    "        self.heas.sort()\n",
    "        self.wavs.sort()\n",
    "        self.tsvs.sort()\n",
    "\n",
    "    # torchaudio로 필터링 적용\n",
    "    def apply_filter_torchaudio(self, audio, cutoff_hz):\n",
    "        biquad = Biquad(typ=Biquad.LOWPASS, freq=cutoff_hz, srate=SAMPLE_RATE, Q=0.707)\n",
    "        b0, b1, b2, a1, a2 = biquad.constants()\n",
    "        b0 = torch.tensor(b0)\n",
    "        b1 = torch.tensor(b1)\n",
    "        b2 = torch.tensor(b2)\n",
    "        a0 = 1\n",
    "        a1 = torch.tensor(a1)\n",
    "        a2 = torch.tensor(a2)\n",
    "        filtered_audio = torchaudio.functional.biquad(audio, b0, b1, b2, a0, a1, a2)\n",
    "        return filtered_audio\n",
    "\n",
    "    def padding(self, spec, target_length, padding_value=0):\n",
    "        pad_width = target_length - spec.shape[-1]\n",
    "        padded_spec = torch.nn.functional.pad(spec, (0, pad_width, 0, 0), \"constant\", 0)\n",
    "        return padded_spec\n",
    "\n",
    "    def resize_spectrogram(self, spec, new_shape):\n",
    "        resized_spec = transforms.functional.resize(img=spec, size=new_shape, antialias=None)\n",
    "        return resized_spec\n",
    "\n",
    "    # # 나누기 전 이미지에 대한 정규화를 위해 사용\n",
    "    # def normalize_spectrogram(self, wav, spec):\n",
    "    #     wav_max = np.max(np.array(wav))\n",
    "    #     spec_max = np.max(np.array(spec))\n",
    "    #     norm_max = spec_max / wav_max\n",
    "    #     print(norm_max)\n",
    "    #     normalized = spec / norm_max\n",
    "    #     return normalized\n",
    "\n",
    "    # 나누어진 개별 이미지의 최대값을 이용한 정규화\n",
    "    def normalize_spectrogram2(self, spec):\n",
    "        spec_max = np.max(np.array(spec))\n",
    "        spec_min = np.min(np.array(spec))\n",
    "        normalized = (spec-spec_min) / (spec_max-spec_min)\n",
    "        return normalized\n",
    "\n",
    "    # # torchvision을 사용한 정규화\n",
    "    # def normalize_spectrogram3(self, spec):\n",
    "    #     normalized = transforms.Normalize((0), (0.5))(spec)\n",
    "    #     return normalized\n",
    "\n",
    "    def get_mel_spectrogram(self):\n",
    "        audio_list = []\n",
    "        self.scale_list = []\n",
    "        self.iter_list = []\n",
    "\n",
    "        for path_wav in self.wavs:\n",
    "            path = os.path.join(PATH, path_wav)\n",
    "\n",
    "            # Torchaudio 이용하여 파일 로드\n",
    "            x = torchaudio.load(path)[0]\n",
    "            # Filtering\n",
    "            if self.filtering is True:\n",
    "                cutoff_frequency = self.filter_hz\n",
    "                x = self.apply_filter_torchaudio(x, cutoff_frequency)\n",
    "            ms = ta_transforms.MelSpectrogram(sample_rate=SAMPLE_RATE,\n",
    "                                        n_fft=128,\n",
    "                                        win_length=100,\n",
    "                                        n_mels=40,\n",
    "                                        hop_length=HOP_LENGTH)(x)\n",
    "\n",
    "            # 스펙트로그램의 형태를 유지하면서 torchaudio로 불러왔을 때와 같이 0~1로 정규화\n",
    "            # ms = self.normalize_spectrogram(x, ms)\n",
    "\n",
    "            # 원본 wav의 길이가 th보다 길다면 Slicing\n",
    "            if ms.shape[-1] > self.th:\n",
    "                scale = 1\n",
    "                num_splits = ms.shape[-1] // self.th    # wav길이 == th의 배수\n",
    "                if ms.shape[-1] % self.th != 0: # wav길이 != th의 배수\n",
    "                    num_splits += 1\n",
    "                self.iter_list.append(num_splits)\n",
    "\n",
    "                for i in range(num_splits):\n",
    "                    start_idx = i * self.th\n",
    "                    end_idx = (i + 1) * self.th\n",
    "                    split = ms[..., start_idx:end_idx]\n",
    "\n",
    "                    # th보다 길이가 짧다면\n",
    "                    if split.shape[-1] < self.th:\n",
    "                        # Resizing\n",
    "                        if self.resizing is True:\n",
    "                            scale = self.th / split.shape[-1]\n",
    "                            target_shape = (split.shape[-2], self.th)\n",
    "                            split = self.resize_spectrogram(split, target_shape)\n",
    "                        # Padding\n",
    "                        else:\n",
    "                            split = self.padding(split, self.th)\n",
    "                    # 최종 Resizing\n",
    "                    resized = self.resize_spectrogram(split, self.target_size)\n",
    "                    resized = self.normalize_spectrogram2(resized)  # 정규화\n",
    "                    audio_list.append(resized)\n",
    "                    if self.resizing is True:\n",
    "                        self.scale_list.append(scale)\n",
    "\n",
    "            # 원본 wav의 길이가 th보다 짧거나 같다면\n",
    "            else:\n",
    "                self.iter_list.append(1)\n",
    "                scale = 1\n",
    "                # th보다 짧다면\n",
    "                if ms.shape[-1] < self.th:\n",
    "                    # Resizing\n",
    "                    if self.resizing is True:\n",
    "                        scale = self.th / ms.shape[-1]\n",
    "                        target_shape = (ms.shape[-2], self.th)\n",
    "                        ms = self.resize_spectrogram(ms, target_shape)\n",
    "                    # Padding\n",
    "                    else:\n",
    "                        ms = self.padding(ms, self.th)\n",
    "                # 최종 resizing\n",
    "                ms = self.resize_spectrogram(ms, self.target_size)\n",
    "                ms = self.normalize_spectrogram2(ms)    # 정규화\n",
    "                audio_list.append(ms)\n",
    "                if self.resizing is True:\n",
    "                    self.scale_list.append(scale)\n",
    "        return torch.stack(audio_list)\n",
    "\n",
    "    def get_label(self):\n",
    "        labels = []\n",
    "        idx=0\n",
    "        for i, path_tsv in enumerate(self.tsvs):\n",
    "            path = os.path.join(PATH, path_tsv)\n",
    "            tsv_data = pd.read_csv(path, sep='\\t', header=None)\n",
    "            iter = self.iter_list[i]\n",
    "            continuous = False\n",
    "            next_end = 0\n",
    "            next_class = 0\n",
    "            for _iter in range(iter):\n",
    "                label = []\n",
    "                if self.resizing is True:\n",
    "                    scale = self.scale_list[sum(self.iter_list[:i]) + _iter]\n",
    "                for _, tsv_row in tsv_data.iterrows():\n",
    "                    # 이전 구간에서 이어진다면 tsv_row[0] = 0\n",
    "                    if continuous is True:\n",
    "                        tsv_row[0] = 0\n",
    "                        tsv_row[1] = next_end\n",
    "                        tsv_row[2] = next_class\n",
    "                        continuous = False\n",
    "                        # resize를 하였다면 라벨 값도 스케일링\n",
    "                        if self.resizing is True:\n",
    "                            tsv_row[0] *= scale\n",
    "                            tsv_row[1] *= scale\n",
    "\n",
    "                    # 이전 구간에서 이어지지 않는다면 새로 데이터 가져오기\n",
    "                    elif tsv_row[2] in [1, 3]:\n",
    "                        # 구간 불러와서 sr값 곱하고 hop_legth로 나누기\n",
    "                        tsv_row[0] = tsv_row[0] * SAMPLE_RATE / HOP_LENGTH - (_iter * self.th)\n",
    "                        tsv_row[1] = tsv_row[1] * SAMPLE_RATE / HOP_LENGTH - (_iter * self.th)\n",
    "                        tsv_row[2] = 0 if tsv_row[2] == 1 else 1    # S1=0, S2=1 \n",
    "                        # 시작점이 th 이상이라면, 이전 구간 이미지의 라벨이라면 continue\n",
    "                        if tsv_row[0] >= self.th or tsv_row[0] < 0:\n",
    "                            continue\n",
    "                        # th보다 길이가 짧다면(나뉜 이미지의 가장 마지막 이미지라면)\n",
    "                        if _iter == iter - 1:\n",
    "                            # resize를 하였다면 라벨 값도 스케일링\n",
    "                            if self.resizing is True:\n",
    "                                tsv_row[0] *= scale\n",
    "                                tsv_row[1] *= scale\n",
    "\n",
    "                        # th보다 길이가 길다면 Slicing\n",
    "                        elif tsv_row[1] >= self.th:\n",
    "                            next_end = tsv_row[1] - self.th\n",
    "                            next_class = tsv_row[2]\n",
    "                            tsv_row[1] = self.th # 지움\n",
    "                            continuous = True\n",
    "                            # 최종 resize한 값 으로 보간\n",
    "                            tsv_row[0] *= self.target_size[1] / self.th\n",
    "                            tsv_row[1] *= self.target_size[1] / self.th\n",
    "                            \n",
    "                            # S1=1, S2=2 \n",
    "                            label.append([tsv_row[0] / self.target_size[0], 0 / self.target_size[0],\n",
    "                                  tsv_row[1] / self.target_size[0], self.target_size[0] / self.target_size[0],\n",
    "                                  int(tsv_row[2])+1])# xmin, ymin, xmax, ymax, cls\n",
    "                            break\n",
    "\n",
    "                    # 이전 값에서 이어지지 않으면서 S1, S2가 아닌 경우 continue\n",
    "                    else: continue\n",
    "\n",
    "                    # 최종 resize한 값 으로 보간\n",
    "                    tsv_row[0] *= self.target_size[1] / self.th\n",
    "                    tsv_row[1] *= self.target_size[1] / self.th\n",
    "                    \n",
    "                    label.append([tsv_row[0] / self.target_size[0], 0 / self.target_size[0],\n",
    "                                  tsv_row[1] / self.target_size[0], self.target_size[0] / self.target_size[0],\n",
    "                                  int(tsv_row[2])+1])# xmin, ymin, xmax, ymax, cls\n",
    "                    \n",
    "                    #label.append((int(tsv_row[2]), tsv_row[0], tsv_row[1]))\n",
    "                if(len(label)==0):\n",
    "                    self.delete_list.append(idx)\n",
    "                idx+=1\n",
    "                labels.append(label)      \n",
    "        return labels\n",
    "    \n",
    "    def delete_data(self):\n",
    "        delete_count=0\n",
    "        for i in self.delete_list:\n",
    "            del self.y[i-delete_count]\n",
    "            delete_count+=1\n",
    "        \n",
    "        self.x = self.x[[i for i in range(self.x.size(0)) if i not in self.delete_list]]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb7e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(PATH, train_patient_txt, target_size=(300, 300), th=5, resizing=True, filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97118475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049 1049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.delete_list)\n",
    "dataset.delete_list\n",
    "print(len(dataset.y),len(dataset.x))\n",
    "dataset.x.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e94b01",
   "metadata": {},
   "source": [
    "# data loader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "482b6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])  # sample[0]은 화상 gt\n",
    "        targets.append(torch.FloatTensor(sample[1]))  # sample[1]은 어노테이션 gt\n",
    "\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    return imgs, targets\n",
    "BATCHSIZE = 8\n",
    "train_dataloader = DataLoader(dataset, batch_size=BATCHSIZE, shuffle=True, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834c6513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "for images, labels in train_dataloader:\n",
    "    print(type(images[0]))\n",
    "    print(labels[0].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271670b5",
   "metadata": {},
   "source": [
    "# model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1451702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#for param in backbone.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd6af982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "avgpool\n",
      "classifier\n",
      "ConvBNActivation(\n",
      "  (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "backbone = models.mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "for name, module in backbone.named_children():\n",
    "    print(name)\n",
    "\n",
    "for n, x in enumerate(backbone.features.children()):\n",
    "    if n==0:\n",
    "        seq=x.children()\n",
    "        seq=next(seq)\n",
    "        \n",
    "        prev_weight = seq.weight\n",
    "        new_weight = prev_weight[:, :1, :, :]\n",
    "        seq.weight = nn.Parameter(new_weight)\n",
    "        seq.in_channels = 1\n",
    "    if n==13:\n",
    "        seq=x.children()\n",
    "        seq=next(seq)\n",
    "        print(seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "baa7d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class SSD(nn.Module):\n",
    "    def __init__(self, backbone, n_class=3, default_box_n=[4,6,6,6,4,4], state = \"Train\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        \n",
    "        self.n_class = n_class\n",
    "        self.default_box_n = default_box_n\n",
    "        self.state = state\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "            \n",
    "        #가중치 초기화 인자\n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 112, 1, 1))  # there are 512 channels in conv4_3_feats\n",
    "        nn.init.constant_(self.rescale_factors, 20)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        #backbone\n",
    "        \n",
    "        self.backbone_layer = nn.Sequential(\n",
    "            backbone.features,\n",
    "        )\n",
    "        \n",
    "        #extra layer\n",
    "        self.extra_layer_1 = self.extra_layers(960,512,4)\n",
    "        self.extra_layer_2 = self.extra_layers(512,256,4)\n",
    "        self.extra_layer_3 = self.extra_layers(256,256,2)\n",
    "        self.extra_layer_4 = self.extra_layers(256,128,2)\n",
    "        \n",
    "        self.extra_layers = [self.extra_layer_1, \n",
    "                            self.extra_layer_2, \n",
    "                            self.extra_layer_3, \n",
    "                            self.extra_layer_4]\n",
    "        \n",
    "        \n",
    "        #detection output\n",
    "        \n",
    "        self.cls_layers = nn.ModuleList([\n",
    "                                        nn.Conv2d(672, default_box_n[0]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(960, default_box_n[1]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(512, default_box_n[2]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(256, default_box_n[3]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(256, default_box_n[4]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(128, default_box_n[5]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                                        ])\n",
    "        \n",
    "        self.loc_layers = nn.ModuleList([\n",
    "                                        nn.Conv2d(672, default_box_n[0]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(960, default_box_n[1]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(512, default_box_n[2]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(256, default_box_n[3]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(256, default_box_n[4]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                                        nn.Conv2d(128, default_box_n[5]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                                        ])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        #가중치 초기화 함수\n",
    "        \n",
    "        #모델학습이 순조롭지않다면 향후 추가 예정\n",
    "        return\n",
    "    \n",
    "    def extra_layers(self, input_size, output_size, div):\n",
    "        layer = nn.Sequential(\n",
    "            #conv2D 해상도낮추기\n",
    "            nn.Conv2d(input_size, output_size, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(output_size, eps=0.001, momentum=0.01, affine=True, track_running_stats=True),\n",
    "            nn.Hardswish(),\n",
    "            \n",
    "            #Inverted Residual (mobilev2 + squeeze)\n",
    "            \n",
    "            #depthwise \n",
    "            # kernel size 5 고려해보기\n",
    "            nn.Conv2d(output_size, output_size, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=output_size, bias=False),\n",
    "            nn.BatchNorm2d(output_size, eps=0.001, momentum=0.01, affine=True, track_running_stats=True),\n",
    "            nn.Hardswish(),\n",
    "            \n",
    "            #SqueezeExcitation\n",
    "            nn.Conv2d(output_size, output_size//div, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_size//div, output_size, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            \n",
    "            #Point-wise\n",
    "            nn.Conv2d(output_size, output_size, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.Hardswish(),\n",
    "            nn.Identity()\n",
    "            \n",
    "        ) \n",
    "        return layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        f_maps=[]\n",
    "        for n, layer in enumerate(backbone.features):#12 16\n",
    "            if n==13: \n",
    "                #L2 norm\n",
    "                #norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()     # (N, 1, 19, 19)\n",
    "                #featureMap_1 = x / norm                                  # (N, 112, 19, 19)\n",
    "                #featureMap_1= conv4_3 * self.rescale_factors            # (N, 512, 19, 19)\n",
    "                \n",
    "                #13계층 bottleneck까지만\n",
    "                seq_layer = next(layer.children())\n",
    "                for seq_n in range(4):\n",
    "                    x = seq_layer[seq_n](x)\n",
    "                    if seq_n==0:\n",
    "                        f_maps.append(x)\n",
    "                continue\n",
    "                \n",
    "            x = layer(x)\n",
    "            #print(\"size : {0}, number = {1}\".format(x.size(), n))\n",
    "            if n==16:\n",
    "                f_maps.append(x) \n",
    "               \n",
    "        #extra layer\n",
    "        mini_batch_n = x.size(0)\n",
    "        for extra_layer in self.extra_layers:\n",
    "            for idx, layer in enumerate(extra_layer.children()):\n",
    "                if mini_batch_n ==1 and (idx == 1 or idx == 4):\n",
    "                    continue\n",
    "                x = layer(x)\n",
    "            \n",
    "            f_maps.append(x)\n",
    "            \n",
    "        cls = []\n",
    "        loc = []\n",
    "        for f_map, cls_layer, loc_layer in zip(f_maps,self. cls_layers, self.loc_layers):\n",
    "            output_cls = cls_layer(f_map)\n",
    "            output_loc = loc_layer(f_map)\n",
    "            \n",
    "            cls.append(output_cls.permute(0, 2, 3, 1).contiguous())\n",
    "            loc.append(output_loc.permute(0, 2, 3, 1).contiguous())\n",
    "            #cls.append(output_cls)\n",
    "            #loc.append(output_loc)\n",
    "            \n",
    "        cls = torch.cat([o.view(o.size(0), -1) for o in cls], 1)\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "\n",
    "        loc = loc.view(loc.size(0), -1, 4)\n",
    "        if self.state == \"Train\":\n",
    "            cls = cls.view(cls.size(0), -1, self.n_class)\n",
    "            #cls = self.softmax(cls.view(cls.size(0), -1, self.n_class))\n",
    "        else:\n",
    "            cls = self.softmax(cls.view(cls.size(0), -1, self.n_class))\n",
    "            \n",
    "            \n",
    "        return cls, loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fcd7e",
   "metadata": {},
   "source": [
    "# default box, loss 함수 기타 기능 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f7f63d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def point_form(boxes):\n",
    "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n",
    "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n",
    "\n",
    "\n",
    "def center_size(boxes):\n",
    "    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n",
    "                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n",
    "\n",
    "\n",
    "def intersect(box_a, box_b):\n",
    "    A = box_a.size(0) #정답개수\n",
    "    B = box_b.size(0) #defalut 박스개수\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2)) #정답, dbox , xmax, ymax\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "\n",
    "def jaccard(box_a, box_b):# iou\n",
    "    inter = intersect(box_a, box_b) # (target_n,dbox_n)\n",
    "    # 확인햇음\n",
    "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
    "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
    "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
    "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]\n",
    "\n",
    "\n",
    "def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n",
    "    #groud truth =target\n",
    "    # jaccard index\n",
    "    # defuatl box와 truths box iou 계산\n",
    "    overlaps = jaccard(\n",
    "        truths,\n",
    "        point_form(priors)\n",
    "    )\n",
    "    \n",
    "    #overlaps.size() = target_n, dbox_n\n",
    "    \n",
    "    # target기준 가장 성능이 좋은 dbox 값 검출, dbox 인덱스 검출\n",
    "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True) #차원, 형태 삭제 x size = target,1\n",
    "    \n",
    "    # dbox 기준 가장 성능이 높았던 target 검출, target 인덱스 검출\n",
    "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True) #size = 1, bbox\n",
    "    \n",
    "    #차원 제거\n",
    "    best_truth_idx.squeeze_(0)\n",
    "    best_truth_overlap.squeeze_(0)\n",
    "    best_prior_idx.squeeze_(1)\n",
    "    best_prior_overlap.squeeze_(1)\n",
    "    #print(best_truth_overlap, best_truth_idx)\n",
    "    #dbox기준 target값을 dbox 인덱스에 해당하면 값을 2로 변경\n",
    "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # 0차원에  best_prior_idx 위치에 value 2를 채움\n",
    "    #1, bbox     fill = target,1 ?????  \n",
    "    for j in range(best_prior_idx.size(0)): \n",
    "        #타겟별 가장 성능이 좋은 dbox의 index 추출\n",
    "        #해당 index의 ddox의 가장 성능이 좋았던 타겟값으로 변경 - 안해도 되는작업이아닌가?\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "        \n",
    "    #print(truths.size())\n",
    "    matches = truths[best_truth_idx]          # Shape: [num_priors,4] #dbox 번호별로 가장 어울리는 타겟값의 정보를 저장함\n",
    "    conf = labels[best_truth_idx]#+1         # Shape: [num_priors] # dbox 번호별로 가장 어울리는 타겟값의 라벨을 저장함\n",
    "    conf[best_truth_overlap < threshold] = 0  # 실제 target값이더라도 dbox들의 iou 값이 낮다면 배경으로 판단\n",
    "    \n",
    "    #해당코드에서 문제가 발생된다고 예상\n",
    "    loc = encode(matches, priors, variances, truths)\n",
    "                            \n",
    "    # idx 는 미니 배치 index\n",
    "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
    "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
    "    \n",
    "\n",
    "def encode(matched, priors, variances, truths):\n",
    "    #torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "    \n",
    "    #print(\"****************************************************\")\n",
    "    eps = 1e-5\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = torch.log(g_wh+eps) / variances[1]\n",
    "\n",
    "    #print(\"****************************************************\")\n",
    "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n",
    "\n",
    "\n",
    "# Adapted from https://github.com/Hakuyume/chainer-ssd\n",
    "def decode(loc, priors, variances):\n",
    "    eps = 1e-5\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    x_max = x.data.max()\n",
    "    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n",
    "\n",
    "\n",
    "def nms(boxes, scores, overlap=0.5, top_k=200):\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0:\n",
    "        return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort in ascending order\n",
    "    # I = I[v >= 0.01]\n",
    "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    # keep = torch.Tensor()\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        # keep.append(i)\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1:\n",
    "            break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= overlap\n",
    "        idx = idx[IoU.le(overlap)]\n",
    "    return keep, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b46eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"SSD의 손실함수 클래스 \"\"\"\n",
    "\n",
    "    def __init__(self, thresh=0.5, neg_pos=3, device='cpu'):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.jaccard_thresh = thresh  # 0.5 match 함수의 jaccard 계수의 임계치\n",
    "        self.negpos_ratio = neg_pos  # 3:1 Hard Negative Mining의 음과 양 비율\n",
    "        self.device = device  # 계산 device(CPU | GPU)\n",
    "\n",
    "    def forward(self, predictions, targets, dboxs):\n",
    "        \"\"\"\n",
    "        파라미터 설명\n",
    "        ----------\n",
    "        predictions :모델의 예측값 cls와 loc\n",
    "        cls는 batch dbox의 개수, 클래스 개수로 이루어짐\n",
    "        loc은 batch dbox의 개수, 4\n",
    "        \n",
    "        targets : [num_batch, 객체개수, 5]\n",
    "            5는 라벨 정보[xmin, ymin, xmax, ymax, label_ind]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        conf_data, loc_data = predictions\n",
    "        dbox_list = dboxs\n",
    "        \n",
    "        num_batch = loc_data.size(0)  # 배치 크기\n",
    "        num_dbox = loc_data.size(1)  # DBox의 수 \n",
    "        num_classes = conf_data.size(2)  # 클래스 수\n",
    "\n",
    "        # 손실 계산에 사용할 것을 저장하는 변수 작성\n",
    "        # conf_t_label：각 DBox에 가장 가까운 정답 BBox의 라벨을 저장 \n",
    "        # loc_t: 각 DBox에 가장 가까운 정답 BBox의 위치 정보 저장 \n",
    "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
    "        \n",
    "        #conf_t_label.fill_(0)#테스트용도\n",
    "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
    "\n",
    "        # loc_t와 conf_t_label에 \n",
    "        # DBox와 정답 어노테이션 targets를 amtch한 결과 덮어쓰기\n",
    "        for idx in range(num_batch):  # 미니 배치 루프\n",
    "\n",
    "            truths = targets[idx][:, :-1].to(self.device)\n",
    "            labels = targets[idx][:, -1].to(self.device)\n",
    "            dbox = dbox_list.to(self.device)\n",
    "\n",
    "            # match 함수를 실행하여 loc_t와 conf_t_label 내용 갱신\n",
    "            # loc_t: 각 DBox에 가장 가까운 정답 BBox 위치 정보가 덮어써짐.\n",
    "            # conf_t_label：각 DBox에 가장 가까운 정답 BBox 라벨이 덮어써짐.\n",
    "            # 단, 가장 가까운 BBox와 iou가 0.5보다 작은 경우,\n",
    "            # 정답 BBox의 라벨 conf_t_label은 배경 클래스 0으로 한다.\n",
    "            variance = [0.1, 0.2]\n",
    "            \n",
    "            # 라벨을 dbox에 대한 offset으로 변환\n",
    "            match(self.jaccard_thresh, truths, dbox,\n",
    "                  variance, labels, loc_t, conf_t_label, idx)\n",
    "\n",
    "\n",
    "        #물체를 발견한 offset만 손실 계산\n",
    "        pos_mask = conf_t_label > 0  # size: batch,dbox,1\n",
    "        true_count = pos_mask[0].sum().item()\n",
    "\n",
    "        # pos_mask를 loc_data 크기로 변형\n",
    "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
    "        \n",
    "        # Positive DBox의 loc_data와 offset loc_t 취득\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4).to(device)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "\n",
    "        # 물체를 발견한 Positive DBox의 오프셋 정보 loc_t의 손실(오차)를 계산\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
    "        # ----------\n",
    "        # 클래스 예측의 손실 : loss_c를 계산\n",
    "        # 교차 엔트로피 오차 함수로 손실 계산. 단 배경 클래스가 정답인 DBox가 압도적으로 많으므로,\n",
    "        # Hard Negative Mining을 실시하여 물체 발견 DBox 및 배경 클래스 DBox의 비율이 1:3이 되도록 한다.\n",
    "        # 배경 클래스 DBox로 예상한 것 중 손실이 적은 것은 클래스 예측 손실에서 제외\n",
    "        # ----------\n",
    "        batch_conf = conf_data.view(-1, num_classes)\n",
    "\n",
    "        # 클래스 예측의 손실함수 계산(reduction='none'으로 하여 합을 취하지 않고 차원 보존)\n",
    "        loss_c = F.cross_entropy(\n",
    "            batch_conf, conf_t_label.view(-1), reduction='none')#batch * dbox_n\n",
    "\n",
    "        \n",
    "        #------------이 아래 부분은 이해 못함 \n",
    "\n",
    "        \n",
    "        \n",
    "        # -----------------\n",
    "        # Negative DBox중 Hard Negative Mining으로 \n",
    "        # 추출하는 것을 구하는 마스크 작성\n",
    "        # -----------------\n",
    "\n",
    "        # 물체를 발견한 Positive DBox의 손실을 0으로 한다.\n",
    "        # (주의) 물체는 label이 1 이상, 라벨 0은 배경을 의미\n",
    "        num_pos = pos_mask.long().sum(1, keepdim=True)  # 미니 배치별 물체 클래스 예측 수\n",
    "        loss_c = loss_c.view(num_batch, -1)  # torch.Size([num_batch, 8732])\n",
    "        loss_c[pos_mask] = 0  # 물체를 발견한 DBox는 손실 0으로 한다.\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        # 각 DBox 손실의 크기 loss_c 순위 idx_rank를 구함\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "\n",
    "        #  (주의) 구현된 코드는 특수하며 직관적이지 않음.\n",
    "        # 위 두 줄의 요점은 각 DBox에 대해 손실 크기가 몇 번째인지의 정보를 \n",
    "        # idx_rank 변수로 빠르게 얻는 코드이다.\n",
    "        \n",
    "        # DBox의 손실 값이 큰 쪽부터 내림차순으로 정렬하여, \n",
    "        # DBox의 내림차순의 index를 loss_idx에 저장한다.\n",
    "        # 손실 크기 loss_c의 순위 idx_rank를 구한다.\n",
    "        # 내림차순이 된 배열 index인 loss_idx를 0부터 8732까지 오름차순으로 다시 정렬하기 위하여\n",
    "        # 몇 번째 loss_idx의 인덱스를 취할 지 나타내는 것이 idx_rank이다.\n",
    "        # 예를 들면 idx_rank 요소의 0번째 = idx_rank[0]을 구하는 것은 loss_idx의 값이 0인 요소,\n",
    "        # 즉 loss_idx[?] =0은 원래 loss_c의 요소 0번째는 내림차순으로 정렬된 loss_idx의 \n",
    "        # 몇 번째입니까? 를구하는 것이 되어 결과적으로, \n",
    "        # ? = idx_rank[0]은 loss_c의 요소 0번째가 내림차순으로 몇 번째인지 나타냄\n",
    "\n",
    "        # 배경 DBox의 수 num_neg를 구한다. HardNegative Mining으로 \n",
    "        # 물체 발견 DBox으 ㅣ수 num_pos의 세 배 (self.negpos_ratio 배)로 한다.\n",
    "        # DBox의 수를 초과한 경우에는 DBox의 수를 상한으로 한다.\n",
    "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\n",
    "\n",
    "        # idx_rank에 각 DBox의 손실 크기가 위에서부터 몇 번째인지 저장되었다.\n",
    "        # 배경 DBox의 수 num_neg보다 순위가 낮은(손실이 큰) DBox를 취하는 마스크 작성\n",
    "        # torch.Size([num_batch, 8732])\n",
    "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
    "\n",
    "        # -----------------\n",
    "        # (종료) 지금부터 Negative DBox 중 Hard Negative Mining으로 추출할 것을 구하는 마스크를 작성\n",
    "        # -----------------\n",
    "\n",
    "        # 마스크 모양을 고쳐 conf_data에 맞춘다\n",
    "        # pos_idx_mask는 Positive DBox의 conf를 꺼내는 마스크이다.\n",
    "        # neg_idx_mask는 Hard Negative Mining으로 추출한 Negative DBox의 conf를 꺼내는 마스크이다.\n",
    "        # pos_mask：torch.Size([num_batch, 8732])\n",
    "        # --> pos_idx_mask：torch.Size([num_batch, 8732, 21])\n",
    "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
    "\n",
    "        # conf_data에서 pos와 neg만 꺼내서 conf_hnm으로 한다. \n",
    "        # 형태는 torch.Size([num_pos+num_neg, 21])\n",
    "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)\n",
    "                             ].view(-1, num_classes)\n",
    "        # gt는 greater than (>)의 약칭. mask가 1인 index를 꺼낸다\n",
    "        # pos_idx_mask+neg_idx_mask는 덧셈이지만 index로 mask를 정리할 뿐임.\n",
    "        # pos이든 neg이든 마스크가 1인 것을 더해 하나의 리스트로 만들어 이를 gt로 췯그한다.\n",
    "\n",
    "        # 마찬가지로 지도 데이터인 conf_t_label에서 pos와 neg만 꺼내, conf_t_label_hnm 으로 \n",
    "        # torch.Size([pos+neg]) 형태가 된다\n",
    "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
    "\n",
    "        # confidence의 손실함수 계산(요소의 합계=sum을 구함)\n",
    "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\n",
    "\n",
    "        # 물체를 발견한 BBox의 수 N (전체 미니 배치의 합계) 으로 손실을 나눈다.\n",
    "        N = num_pos.sum()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        \n",
    "        #print(\"-\"*100)\n",
    "        \n",
    "        return loss_l, loss_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "77883c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt\n",
    "from itertools import product as product\n",
    "\n",
    "class default:\n",
    "    \n",
    "    def __init__(self, image_size=300, feature_maps=[19, 10, 5, 3, 2, 1], min_sizes=[]):\n",
    "        super(default, self).__init__()\n",
    "        self.image_size = 300\n",
    "        # number of priors for feature map location (either 4 or 6)\n",
    "        self.feature_maps = feature_maps\n",
    "        self.min_sizes = [16, 30, 60, 100, 150, 300]  #0.2, 0.34, 0.48, 0.62, 0.76, 0.9?\n",
    "        self.max_sizes = [30, 60, 100, 150, 300, 300]\n",
    "        self.steps = [19,10,5,3,2,1] # 이미지 그리드로 나눈 개수\n",
    "        self.aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
    "        self.clip = True\n",
    "\n",
    "    def forward(self):\n",
    "        mean = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):\n",
    "                f_k = self.steps[k] # 그리드 개수\n",
    "                # default 박스 중점\n",
    "                cx = (j + 0.5) / f_k\n",
    "                cy = (i + 0.5) / f_k\n",
    "                # aspect_ratio: 1\n",
    "                # default 박스 공식이 아닌 임의로 설정 그리드 크기를 기준으로 나눔\n",
    "                s_k = self.min_sizes[k]/self.image_size\n",
    "                mean += [cx, cy, s_k, 0.7] # s_k\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # s_k*s_k+1\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
    "                \n",
    "                #print(cx,cy,s_k_prime,self.max_sizes[k]/self.image_size)\n",
    "                mean += [cx, cy, s_k_prime, 0.7] #s_k\n",
    "\n",
    "                # rest of aspect ratios\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    mean += [cx, cy, s_k*sqrt(ar), 0.7] # s_k/sqrt(ar)\n",
    "                    mean += [cx, cy, s_k/sqrt(ar), 0.7]\n",
    "        # back to torch land\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output\n",
    "    def cxcy_to_xy(self, tensor_default_box):\n",
    "        return tensor_default_box*self.image_size\n",
    "    \n",
    "    def xy_to_cxcy(self, tensor_default_box):\n",
    "        return tensor_default_box/self.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eda2ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "def train_step(model, Data_loader, epoch_num, lr=0.005, is_wandb=False):\n",
    "    if is_wandb==True:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"Heart_Signal_Detection\",\n",
    "\n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": \"Mobilenet_v3 + SSD\",\n",
    "            \"dataset\": \"circor-heart-sound\",\n",
    "            \"epochs\": epoch_num,\n",
    "            \"batch\" : BATCHSIZE \n",
    "            }\n",
    "        )\n",
    "        \n",
    "    d=default()\n",
    "    tensor_d = d.forward()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_func = MultiBoxLoss(device=device)\n",
    "    \n",
    "    epoch_train_loss = 0.0 # 에포크 손실 합\n",
    "    #epoch_val_loss = 0.0\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start = time.time()\n",
    "        iter_start = time.time()\n",
    "        print(\"Epoch : {0} / {1}\".format(epoch+1,epoch_num))\n",
    "        \n",
    "        #targets=np.array([[[1,2,3,4,5],[1,2,3,4,5]],[[1,2,3,4,5],[1,2,3,4,5]]])\n",
    "        for idx, data in enumerate(Data_loader):\n",
    "            \n",
    "            #img=torch.zeros((32,3,300,300),dtype=torch.float)\n",
    "\n",
    "            images = data[0].to(device)\n",
    "\n",
    "            labels = [label.to(device) for label in data[1]]\n",
    "            optimizer.zero_grad()#이전 값 들에 대한 가중치 기울기 초기화\n",
    "            with torch.set_grad_enabled(True):\n",
    "                cls, loc = model(images)\n",
    "                tensor_d = tensor_d.to(device)\n",
    "                loss_l, loss_c = loss_func((cls, loc), labels, tensor_d)\n",
    "                \n",
    "                loss = loss_l + loss_c\n",
    "                loss.backward()\n",
    "                \n",
    "                nn.utils.clip_grad_value_(model.parameters(), clip_value=2.0)\n",
    "                        \n",
    "                \n",
    "                optimizer.step() # 파라미터 갱신\n",
    "            if(idx % 50 == 0):\n",
    "                iter_end = time.time()\n",
    "                print(\"Current Batch {0} / {1} | Cls Loss : {2:.3f}, Loc Loss : {3:.3f}, Total Loss : {4:.3f} | 10 iter time {5:.4f}: \"\n",
    "                      .format(idx, len(Data_loader), loss_l.item(), loss_c.item(), loss.item(), iter_end - iter_start))\n",
    "                \n",
    "                if is_wandb==True:\n",
    "                    wandb.log({\"total_loss\": loss.item(),\n",
    "                               \"Cls_loss\": loss_c.item(),\n",
    "                               \"Loc_loss\": loss_l.item()})\n",
    "                \n",
    "                iter_start =time.time()\n",
    "                \n",
    "            epoch_train_loss+=loss.item()\n",
    "            \n",
    "        epoch_end = time.time()        \n",
    "        print(\"Epoch : {0} / {1} of Total Loss : Total Loss : {2:.3f} | 1 epoch update time : {3:.2f}s\"\n",
    "              .format(epoch+1, epoch_num, epoch_train_loss,epoch_end-epoch_start))\n",
    "        print(\"-----------------------------------------------\")\n",
    "        epoch_train_loss=0\n",
    "\n",
    "        torch.save(model, \n",
    "                './objectdetection_model/ssd300_' + str(epoch+1) + '.pth')\n",
    "        torch.save(model.state_dict(), \n",
    "                './objectdetection_model/ssd300_weight_' + str(epoch+1) + '.pth')\n",
    "    if is_wandb==True:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6651362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(inference_type, input_channels=1):\n",
    "\n",
    "    backbone = models.mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "    for name, layers in backbone.named_parameters():\n",
    "        if name.split(\".\")[1]!=\"0\":\n",
    "            layers.requires_grad = False\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #for name, layers in backbone.named_children():\n",
    "    #    for param in layers.parameters():\n",
    "    #        param.requires_grad = False\n",
    "\n",
    "    #for name, module in backbone.named_children():\n",
    "    #    print(name)\n",
    "    \n",
    "    for n, x in enumerate(backbone.features.children()):\n",
    "        if n==0:\n",
    "            seq=x.children()\n",
    "            seq=next(seq)\n",
    "\n",
    "            prev_weight = seq.weight\n",
    "            new_weight = prev_weight[:, :1, :, :]\n",
    "            seq.weight = nn.Parameter(new_weight)\n",
    "            seq.in_channels = 1\n",
    "        if n==13:\n",
    "            seq=x.children()\n",
    "            seq=next(seq)\n",
    "    model = SSD(backbone, n_class=3, state = inference_type)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6327d1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.15.10-py3-none-any.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 7.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.36-py3-none-any.whl (189 kB)\n",
      "     |████████████████████████████████| 189 kB 56.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb) (4.0.1)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n",
      "     |████████████████████████████████| 224 kB 68.5 MB/s            \n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "     |████████████████████████████████| 62 kB 1.9 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "     |████████████████████████████████| 143 kB 56.2 MB/s            \n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=1bcc2b23a98820591a88c5160b1d3691423ba060f0fe4786da39c2523f5f23a5\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built pathtools\n",
      "Installing collected packages: smmap, urllib3, gitdb, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.7\n",
      "    Uninstalling urllib3-1.26.7:\n",
      "      Successfully uninstalled urllib3-1.26.7\n",
      "Successfully installed GitPython-3.1.36 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.31.0 setproctitle-1.3.2 smmap-5.0.0 urllib3-1.26.16 wandb-0.15.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "#517341de49da22c728d72c1176c5d947ddf49f8c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5da55fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35cc1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "27518a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 / 30\n",
      "Current Batch 0 / 132 | Cls Loss : 5.620, Loc Loss : 7.088, Total Loss : 12.708 | 10 iter time 0.1437: \n",
      "Current Batch 50 / 132 | Cls Loss : 2.038, Loc Loss : 6.178, Total Loss : 8.216 | 10 iter time 5.7147: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106/1798253833.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_wandb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_106/719284513.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, Data_loader, epoch_num, lr, is_wandb)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#이전 값 들에 대한 가중치 기울기 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mtensor_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mloss_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_106/2921520590.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;31m#print(\"size : {0}, number = {1}\".format(x.size(), n))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/models/mobilenetv3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = build_model(\"Train\")\n",
    "backbone = models.mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "for name, layers in backbone.named_parameters():\n",
    "    if name.split(\".\")[1]!=\"0\":\n",
    "        layers.requires_grad = False\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for n, x in enumerate(backbone.features.children()):\n",
    "    if n==0:\n",
    "        seq=x.children()\n",
    "        seq=next(seq)\n",
    "\n",
    "        prev_weight = seq.weight\n",
    "        new_weight = prev_weight[:, :1, :, :]\n",
    "        seq.weight = nn.Parameter(new_weight)\n",
    "        seq.in_channels = 1\n",
    "    if n==13:\n",
    "        seq=x.children()\n",
    "        seq=next(seq)\n",
    "model = SSD(backbone, n_class=3)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_step(model, train_dataloader, epoch_num = 30, is_wandb=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cc70e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Detect:\n",
    "    \"\"\"At test time, Detect is the final layer of SSD.  Decode location preds,\n",
    "    apply non-maximum suppression to location predictions based on conf\n",
    "    scores and threshold to a top_k number of output predictions for both\n",
    "    confidence score and locations.\n",
    "    \"\"\"\n",
    "        \n",
    "    def forward(self, loc_data, conf_data, prior_data, num_classes, bkg_label=0, top_k=30, conf_thresh=0.4, nms_thresh=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loc_data: (tensor) Loc preds from loc layers\n",
    "                Shape: [batch,num_priors*4]\n",
    "            conf_data: (tensor) Shape: Conf preds from conf layers\n",
    "                Shape: [batch*num_priors,num_classes]\n",
    "            prior_data: (tensor) Prior boxes and variances from priorbox layers\n",
    "                Shape: [1,num_priors,4]\n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.background_label = bkg_label\n",
    "        self.top_k = top_k\n",
    "        # Parameters used in nms.\n",
    "        self.nms_thresh = nms_thresh\n",
    "        if nms_thresh <= 0:\n",
    "            raise ValueError('nms_threshold must be non negative.')\n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.variance = [0.1, 0.2]\n",
    "        \n",
    "        num = loc_data.size(0)  # batch size\n",
    "        num_priors = prior_data.size(0)\n",
    "        output = torch.zeros(num, self.num_classes, self.top_k, 5)\n",
    "        \n",
    "        conf_preds = conf_data.view(num, num_priors,\n",
    "                                    self.num_classes).transpose(2, 1) # batch, num_class, dbox_num 클래스별 confidence 취득\n",
    "        # Decode predictions into bboxes.\n",
    "        for i in range(num): #배치별 반복\n",
    "            decoded_boxes = decode(loc_data[i], prior_data, self.variance)#xmin,xmax,ymin,ymax 변환\n",
    "            # For each class, perform nms\n",
    "            conf_scores = conf_preds[i].clone() # (num_class, dbox)\n",
    "            for cl in range(1, self.num_classes):#클래스별 confidence계산\n",
    "                c_mask = conf_scores[cl].gt(self.conf_thresh) #해당값 이하면 false 이상이면 True , size= (dbox)\n",
    "                scores = conf_scores[cl][c_mask] #인식된 default box의 점수만 남음\n",
    "\n",
    "                if scores.size(0) == 0: #인식된 개체 없으면 넘어감\n",
    "                    continue\n",
    "                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes) # dbox,1 -> dbox,4\n",
    "                boxes = decoded_boxes[l_mask].view(-1, 4)\n",
    "                # idx of highest scoring and non-overlapping boxes per class\n",
    "                ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n",
    "                output[i, cl, :count] = \\\n",
    "                    torch.cat((scores[ids[:count]].unsqueeze(1),\n",
    "                               boxes[ids[:count]]), 1)\n",
    "        flt = output.contiguous().view(num, -1, 5)\n",
    "        _, idx = flt[:, :, 0].sort(1, descending=True)\n",
    "        _, rank = idx.sort(1)\n",
    "        flt[(rank < self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3ab4aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def get_iou(a,b):\n",
    "    if len(a)!=4 or len(b)!=4:\n",
    "        return 0\n",
    "    a_area = (a[2]-a[0])*(a[3]-a[1])\n",
    "    b_area = (b[2]-b[0])*(b[3]-b[1])\n",
    "    \n",
    "    union_area = a_area + b_area\n",
    "    \n",
    "    x1 = max(a[0], b[0])\n",
    "    x2 = min(a[2], b[2])\n",
    "    \n",
    "    y1 = max(a[1], b[1])\n",
    "    y2 = min(a[3], b[3])\n",
    "    \n",
    "    iter_area=0\n",
    "    w = x2-x1\n",
    "    h = y2-y1\n",
    "    if w > 0 and h > 0:\n",
    "        iter_area = w*h\n",
    "    \n",
    "    return iter_area / (union_area+iter_area)\n",
    "\n",
    "def get_correct_ration(det_bboxes, target_bboxes, batch, n_class, iou_threshold = 0.2):\n",
    "    '''\n",
    "    det_bbox \n",
    "    type:list\n",
    "    (cls, batch) data= numpy(검출한 객체개수, 5) xmin, ymin, xmax, ymax, cls(confidence)\n",
    "    \n",
    "    target_bbox\n",
    "    type : list\n",
    "    (batch,객체개수,5) xmin, ymin, xmax, ymax, cls\n",
    "    정규화 되어있음\n",
    "    '''\n",
    "    TPFN=[[[-1, \"FN\", -1] for _ in t_bboxes] for t_bboxes in target_bboxes]  #tp, fn 저장용 실측 기준\n",
    "    TPFP=[[[[b_cls_bbox[-1], \"FP\"] for b_cls_bbox in b_cls_bboxes] for b_cls_bboxes in cls_bboxes] for cls_bboxes in det_bboxes]  #tp, fp 저장용 예측 기준\n",
    "\n",
    "    for i in range(batch):\n",
    "        for cl in range(1,n_class):\n",
    "            t_bboxes = np.array(target_bboxes[i])#객체개수, 5\n",
    "            d_bboxes = det_bboxes[cl][i]\n",
    "            for t_i, t_bbox in enumerate(t_bboxes):\n",
    "                if t_bbox[-1]==cl:\n",
    "                    for d_i, d_bbox in enumerate(d_bboxes):\n",
    "                        iou = get_iou(d_bbox[:-1],t_bbox[:-1])\n",
    "                        if iou >= iou_threshold:\n",
    "                            if TPFN[i][t_i][1]=='TP' and d_bbox[-1] > TPFN[i][t_i][0]: # 더 높은 iou를 가진 값이 아닌 confidenc를 우선순위로\n",
    "                                TPFN[i][t_i] = [d_bbox[-1],\"TP\", d_i]\n",
    "                                TPFP[cl][i][d_i] = [d_bbox[-1],\"TP\"]\n",
    "                                TPFP[cl][i][d_i][1] = \"FP\"\n",
    "                                \n",
    "                            elif TPFN[i][t_i][1]=='FN':\n",
    "                                TPFN[i][t_i] = [d_bbox[-1],\"TP\", d_i]\n",
    "                                TPFP[cl][i][d_i] = [d_bbox[-1],\"TP\"]\n",
    "                                \n",
    "                                \n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "    return TPFN, TPFP\n",
    "def test_step(model, Data_loader, detect, image_size=(300,300)):\n",
    "\n",
    "    d=default()\n",
    "    tensor_d = d.forward()\n",
    "    tensor_d = tensor_d.to(device)  \n",
    "    model.to(device)\n",
    "    \n",
    "    w, h = image_size #임시\n",
    "    ap=[]\n",
    "    total_Recall = 0\n",
    "    total_Precison = 0\n",
    "    N=0\n",
    "    for idx, data in enumerate(Data_loader):\n",
    "          \n",
    "        images = data[0].to(device)\n",
    "        labels = [label.cpu() for label in data[1]] # batch, 객체개수, 5\n",
    "        \n",
    "        total_labels=0\n",
    "        for label in labels:\n",
    "            total_labels+=len(label)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cls, loc = model(images)\n",
    "            #상위 200개에 대한 detection\n",
    "            #size=(batch, numclass ,200, 5)\n",
    "            output = detect.forward(loc, cls, tensor_d, cls.size(-1))\n",
    "        \n",
    "        all_boxes = [[[] for _ in range(output.size(0))]\n",
    "                 for _ in range(output.size(1))]  #all_boxes[cls][image]\n",
    "        \n",
    "        \n",
    "        for i in range(output.size(0)):\n",
    "            for j in range(1, output.size(1)):#class 종류\n",
    "                dets = output[i, j, :]\n",
    "                mask = dets[:, 0].gt(0.).expand(5, dets.size(0)).t()#confidence < 0\n",
    "                dets = torch.masked_select(dets, mask).view(-1, 5)\n",
    "                if dets.size(0) == 0: #감지된게 없으면\n",
    "                    continue\n",
    "                boxes = dets[:, 1:]\n",
    "                scores = dets[:, 0].cpu().numpy()\n",
    "\n",
    "                cls_dets = np.hstack((boxes.cpu().numpy(),\n",
    "                                      scores[:, np.newaxis])).astype(np.float32,\n",
    "                                                                     copy=False)\n",
    "                all_boxes[j][i] = cls_dets\n",
    "\n",
    "        TPFN,TPFP = get_correct_ration(all_boxes, labels, batch = output.size(0), n_class = output.size(1), iou_threshold = 0.2)\n",
    "        \n",
    "        total_TP_1 = 0\n",
    "        total_TP_2 = 0 #검수용\n",
    "        total_FN = 0\n",
    "        total_FP = 0\n",
    "        \n",
    "        for batch_TPFN in TPFN:\n",
    "            for real_det in batch_TPFN:\n",
    "                con, state, _ = real_det\n",
    "                if state==\"FN\":\n",
    "                    total_FN+=1\n",
    "                    \n",
    "                if state==\"TP\":\n",
    "                    total_TP_1+=1\n",
    "                    \n",
    "        TPFP_cls_filter=[[] for _ in range(output.size(1))] #n_class\n",
    "        TPFP_filter=[] # total AP 계산용\n",
    "        for cls_idx, cls_TPFP in enumerate(TPFP):\n",
    "            if cls_idx == 0:\n",
    "                continue\n",
    "            TPFP_cls_list=[]\n",
    "            for batch_TPFP in cls_TPFP:\n",
    "                for pred_det in batch_TPFP:\n",
    "                    if len(pred_det) == 0:\n",
    "                        continue\n",
    "                    con, state = pred_det\n",
    "                    \n",
    "                    if state==\"FP\":\n",
    "                        total_FP+=1\n",
    "                    \n",
    "                    if state==\"TP\":\n",
    "                        total_TP_2+=1\n",
    "                        \n",
    "                    TPFP_cls_list.append(pred_det)\n",
    "                    TPFP_filter.append(pred_det)\n",
    "                    \n",
    "            TPFP_cls_filter[cls_idx] = TPFP_cls_list\n",
    "        \n",
    "        \n",
    "        Precison = total_TP_1 / (total_TP_1 + total_FP) \n",
    "        Recall = total_TP_1 / (total_TP_1 + total_FN) #total_labels\n",
    "     \n",
    "        total_Recall += Recall\n",
    "        total_Precison += Precison\n",
    "        N = idx+1\n",
    "        \n",
    "    total_Recall /= N\n",
    "    total_Precison /= N\n",
    "    \n",
    "    return total_Recall, total_Precison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b32b42d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = build_model(\"Test\")\n",
    "weight = torch.load('./objectdetection_model/ssd300_weight_30' + '.pth')\n",
    "model.load_state_dict(weight)\n",
    "model.eval()\n",
    "func_detect = Detect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "88121fbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "0.015384615384615385 0.01282051282051282\n",
      "0.020833333333333332 0.01098901098901099\n",
      "0.05263157894736842 0.014184397163120567\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.02702702702702703 0.009433962264150943\n",
      "0.02631578947368421 0.014285714285714285\n",
      "0.02702702702702703 0.010752688172043012\n",
      "0.0 0.0\n",
      "0.022727272727272728 0.009009009009009009\n",
      "0.0 0.0\n",
      "0.023255813953488372 0.01\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0392156862745098 0.022727272727272728\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.01818181818181818 0.00847457627118644\n",
      "0.0 0.0\n",
      "0.09090909090909091 0.017543859649122806\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.041666666666666664 0.023809523809523808\n",
      "0.03636363636363636 0.0196078431372549\n",
      "0.0 0.0\n",
      "0.06 0.02857142857142857\n",
      "0.046511627906976744 0.0196078431372549\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.029411764705882353 0.009523809523809525\n",
      "0.022727272727272728 0.010309278350515464\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.02631578947368421 0.008620689655172414\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.02857142857142857 0.007633587786259542\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.017857142857142856 0.007751937984496124\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.027777777777777776 0.007462686567164179\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.023255813953488372 0.010416666666666666\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.045454545454545456 0.02\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.030303030303030304 0.008771929824561403\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.023809523809523808 0.011904761904761904\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.02631578947368421 0.008130081300813009\n",
      "0.02702702702702703 0.009615384615384616\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.019230769230769232 0.01020408163265306\n",
      "0.0 0.0\n",
      "0.015384615384615385 0.009433962264150943\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.01818181818181818 0.011235955056179775\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.05 0.023255813953488372\n",
      "0.03508771929824561 0.020202020202020204\n",
      "0.024390243902439025 0.01020408163265306\n",
      "0.022727272727272728 0.009174311926605505\n",
      "0.0 0.0\n",
      "0.02040816326530612 0.00909090909090909\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.018518518518518517 0.008403361344537815\n",
      "0.020833333333333332 0.009615384615384616\n",
      "0.015873015873015872 0.012048192771084338\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "recall, precison = test_step(model, train_dataloader, detect = func_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a3cb4523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0036729282626960422\n"
     ]
    }
   ],
   "source": [
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bdca44e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106/2130048614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "label=[[1,2,3],[1,2]]\n",
    "label[:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b4a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
