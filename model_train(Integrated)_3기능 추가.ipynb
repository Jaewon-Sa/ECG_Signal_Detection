{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0b943266",
      "metadata": {
        "id": "0b943266"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, lfilter\n",
        "from scipy.io import wavfile\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.transform import resize\n",
        "from torchvision import transforms\n",
        "import torchaudio.transforms as ta_transforms\n",
        "import math\n",
        "import torchaudio\n",
        "import time\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0f1fda59",
      "metadata": {
        "id": "0f1fda59"
      },
      "outputs": [],
      "source": [
        "PATH=os.getenv(\"HOME\")+\"/aiffel/ECG_data/physionet.org/files/circor-heart-sound/1.0.3/training_data\"\n",
        "SAMPLE_RATE = 4000\n",
        "HOP_LENGTH = 40"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# txt파일 불러오기\n",
        "file_list = os.listdir(PATH)\n",
        "txt_list = [os.path.join(PATH, file) for file in file_list if file.endswith(\".txt\")]\n",
        "\n",
        "# 환자 아이디를 훈련 데이터셋과 테스트 데이터셋으로 나눔\n",
        "train_patient_txt, test_patient_txt = train_test_split(txt_list, test_size=0.9, random_state=42)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Train Patient IDs:\", train_patient_txt[:3])\n",
        "print(\"Test Patient IDs:\", test_patient_txt[:3])"
      ],
      "metadata": {
        "id": "2LUkvrGj8WvE"
      },
      "id": "2LUkvrGj8WvE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f92c1e8a",
      "metadata": {
        "id": "f92c1e8a"
      },
      "outputs": [],
      "source": [
        "class Biquad:\n",
        "\n",
        "  # pretend enumeration\n",
        "    LOWPASS, HIGHPASS, BANDPASS, NOTCH, PEAK, LOWSHELF, HIGHSHELF = range(7)\n",
        "\n",
        "    def __init__(self,typ, freq, srate, Q, dbGain = 0):\n",
        "        types = {\n",
        "            Biquad.LOWPASS : self.lowpass,\n",
        "            Biquad.HIGHPASS : self.highpass,\n",
        "            Biquad.BANDPASS : self.bandpass,\n",
        "            Biquad.NOTCH : self.notch,\n",
        "            Biquad.PEAK : self.peak,\n",
        "            Biquad.LOWSHELF : self.lowshelf,\n",
        "            Biquad.HIGHSHELF : self.highshelf\n",
        "          }\n",
        "        assert typ in types\n",
        "        freq = float(freq)\n",
        "        self.srate = float(srate)\n",
        "        Q = float(Q)\n",
        "        dbGain = float(dbGain)\n",
        "        self.a0 = self.a1 = self.a2 = 0\n",
        "        self.b0 = self.b1 = self.b2 = 0\n",
        "        self.x1 = self.x2 = 0\n",
        "        self.y1 = self.y2 = 0\n",
        "        # only used for peaking and shelving filter types\n",
        "        A = math.pow(10, dbGain / 40)\n",
        "        omega = 2 * math.pi * freq / self.srate\n",
        "        sn = math.sin(omega)\n",
        "        cs = math.cos(omega)\n",
        "        alpha = sn / (2*Q)\n",
        "        beta = math.sqrt(A + A)\n",
        "        types[typ](A,omega,sn,cs,alpha,beta)\n",
        "        # prescale constants\n",
        "        self.b0 /= self.a0\n",
        "        self.b1 /= self.a0\n",
        "        self.b2 /= self.a0\n",
        "        self.a1 /= self.a0\n",
        "        self.a2 /= self.a0\n",
        "\n",
        "    def lowpass(self,A,omega,sn,cs,alpha,beta):\n",
        "        self.b0 = (1 - cs) /2\n",
        "        self.b1 = 1 - cs\n",
        "        self.b2 = (1 - cs) /2\n",
        "        self.a0 = 1 + alpha\n",
        "        self.a1 = -2 * cs\n",
        "        self.a2 = 1 - alpha\n",
        "\n",
        "    def highpass(self,A,omega,sn,cs,alpha,beta):\n",
        "        self.b0 = (1 + cs) /2\n",
        "        self.b1 = -(1 + cs)\n",
        "        self.b2 = (1 + cs) /2\n",
        "        self.a0 = 1 + alpha\n",
        "        self.a1 = -2 * cs\n",
        "        self.a2 = 1 - alpha\n",
        "\n",
        "    def bandpass(self,A,omega,sn,cs,alpha,beta):\n",
        "        self.b0 = alpha\n",
        "        self.b1 = 0\n",
        "        self.b2 = -alpha\n",
        "        self.a0 = 1 + alpha\n",
        "        self.a1 = -2 * cs\n",
        "        self.a2 = 1 - alpha\n",
        "\n",
        "    def notch(self,A,omega,sn,cs,alpha,beta):\n",
        "        self.b0 = 1\n",
        "        self.b1 = -2 * cs\n",
        "        self.b2 = 1\n",
        "        self.a0 = 1 + alpha\n",
        "        self.a1 = -2 * cs\n",
        "        self.a2 = 1 - alpha\n",
        "\n",
        "    def peak(self,A,omega,sn,cs,alpha,beta):\n",
        "        self.b0 = 1 + (alpha * A)\n",
        "        self.b1 = -2 * cs\n",
        "        self.b2 = 1 - (alpha * A)\n",
        "        self.a0 = 1 + (alpha /A)\n",
        "        self.a1 = -2 * cs\n",
        "        self.a2 = 1 - (alpha /A)\n",
        "\n",
        "    def lowshelf(self,A,omega,sn,cs,alpha,beta):\n",
        "        self.b0 = A * ((A + 1) - (A - 1) * cs + beta * sn)\n",
        "        self.b1 = 2 * A * ((A - 1) - (A + 1) * cs)\n",
        "        self.b2 = A * ((A + 1) - (A - 1) * cs - beta * sn)\n",
        "        self.a0 = (A + 1) + (A - 1) * cs + beta * sn\n",
        "        self.a1 = -2 * ((A - 1) + (A + 1) * cs)\n",
        "        self.a2 = (A + 1) + (A - 1) * cs - beta * sn\n",
        "\n",
        "    def highshelf(self,A,omega,sn,cs,alpha,beta):\n",
        "        self.b0 = A * ((A + 1) + (A - 1) * cs + beta * sn)\n",
        "        self.b1 = -2 * A * ((A - 1) + (A + 1) * cs)\n",
        "        self.b2 = A * ((A + 1) + (A - 1) * cs - beta * sn)\n",
        "        self.a0 = (A + 1) - (A - 1) * cs + beta * sn\n",
        "        self.a1 = 2 * ((A - 1) - (A + 1) * cs)\n",
        "        self.a2 = (A + 1) - (A - 1) * cs - beta * sn\n",
        "\n",
        "  # perform filtering function\n",
        "    def __call__(self,x):\n",
        "        y = self.b0 * x + self.b1 * self.x1 + self.b2 * self.x2 - self.a1 * self.y1 - self.a2 * self.y2\n",
        "        self.x2, self.x1 = self.x1, x\n",
        "        self.y2, self.y1 = self.y1, y\n",
        "\n",
        "        return y\n",
        "\n",
        "  # provide a static result for a given frequency f\n",
        "    def result(self,f):\n",
        "        phi = (math.sin(math.pi * f * 2/(2.0 * self.srate)))**2\n",
        "        return ((self.b0+self.b1+self.b2)**2 - \\\n",
        "        4*(self.b0*self.b1 + 4*self.b0*self.b2 + \\\n",
        "        self.b1*self.b2)*phi + 16*self.b0*self.b2*phi*phi) / \\\n",
        "        ((1+self.a1+self.a2)**2 - 4*(self.a1 + \\\n",
        "        4*self.a2 + self.a1*self.a2)*phi + 16*self.a2*phi*phi)\n",
        "\n",
        "    def log_result(self,f):\n",
        "        try:\n",
        "            r = 10 * math.log10(self.result(f))\n",
        "        except:\n",
        "            r = -200\n",
        "        return r\n",
        "\n",
        "  # return computed constants\n",
        "    def constants(self):\n",
        "        return self.b0,self.b1,self.b2,self.a1,self.a2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7c27e9aa",
      "metadata": {
        "id": "7c27e9aa"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, path, txt_list,\n",
        "                 target_size=(40, 2500),\n",
        "                 th=25,\n",
        "                 resizing=False,\n",
        "                 filtering=False,\n",
        "                 filter_hz=500):\n",
        "        self.path = path\n",
        "        self.txt_list = txt_list\n",
        "        self.target_size = target_size\n",
        "        self.th = int(th * SAMPLE_RATE / HOP_LENGTH)\n",
        "        self.resizing = resizing\n",
        "        self.filtering = filtering\n",
        "        self.filter_hz = filter_hz\n",
        "\n",
        "        self.get_file_list()\n",
        "\n",
        "        self.delete_list=[]\n",
        "        self.x = self.get_mel_spectrogram()\n",
        "        self.y = self.get_label()\n",
        "        self.delete_data()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "    def get_file_list(self):\n",
        "        self.heas = []\n",
        "        self.wavs = []\n",
        "        self.tsvs = []\n",
        "\n",
        "        for path_txt in self.txt_list:\n",
        "            with open(path_txt, \"r\") as f:\n",
        "                P_id, n, sr = f.readline().split()\n",
        "                for _ in range(int(n)):\n",
        "                    _, hea, wav, tsv = f.readline().split()\n",
        "                    self.heas.append(hea)\n",
        "                    self.wavs.append(wav)\n",
        "                    self.tsvs.append(tsv)\n",
        "        self.heas.sort()\n",
        "        self.wavs.sort()\n",
        "        self.tsvs.sort()\n",
        "\n",
        "    # torchaudio로 필터링 적용\n",
        "    def apply_filter_torchaudio(self, audio, cutoff_hz):\n",
        "        biquad = Biquad(typ=Biquad.LOWPASS, freq=cutoff_hz, srate=SAMPLE_RATE, Q=0.707)\n",
        "        b0, b1, b2, a1, a2 = biquad.constants()\n",
        "        b0 = torch.tensor(b0)\n",
        "        b1 = torch.tensor(b1)\n",
        "        b2 = torch.tensor(b2)\n",
        "        a0 = 1\n",
        "        a1 = torch.tensor(a1)\n",
        "        a2 = torch.tensor(a2)\n",
        "        filtered_audio = torchaudio.functional.biquad(audio, b0, b1, b2, a0, a1, a2)\n",
        "        return filtered_audio\n",
        "\n",
        "    def padding(self, spec, target_length, padding_value=0):\n",
        "        pad_width = target_length - spec.shape[-1]\n",
        "        padded_spec = torch.nn.functional.pad(spec, (0, pad_width, 0, 0), \"constant\", 0)\n",
        "        return padded_spec\n",
        "\n",
        "    def resize_spectrogram(self, spec, new_shape):\n",
        "        resized_spec = transforms.functional.resize(img=spec, size=new_shape, antialias=None)\n",
        "        return resized_spec\n",
        "\n",
        "    # # 나누기 전 이미지에 대한 정규화를 위해 사용\n",
        "    # def normalize_spectrogram(self, wav, spec):\n",
        "    #     wav_max = np.max(np.array(wav))\n",
        "    #     spec_max = np.max(np.array(spec))\n",
        "    #     norm_max = spec_max / wav_max\n",
        "    #     print(norm_max)\n",
        "    #     normalized = spec / norm_max\n",
        "    #     return normalized\n",
        "\n",
        "    # 나누어진 개별 이미지의 최대값을 이용한 정규화\n",
        "    def normalize_spectrogram2(self, spec):\n",
        "        spec_max = np.max(np.array(spec))\n",
        "        spec_min = np.min(np.array(spec))\n",
        "        normalized = (spec-spec_min) / (spec_max-spec_min)\n",
        "        return normalized\n",
        "\n",
        "    # # torchvision을 사용한 정규화\n",
        "    # def normalize_spectrogram3(self, spec):\n",
        "    #     normalized = transforms.Normalize((0), (0.5))(spec)\n",
        "    #     return normalized\n",
        "\n",
        "    def get_mel_spectrogram(self):\n",
        "        audio_list = []\n",
        "        self.scale_list = []\n",
        "        self.iter_list = []\n",
        "\n",
        "        for path_wav in self.wavs:\n",
        "            path = os.path.join(PATH, path_wav)\n",
        "\n",
        "            # Torchaudio 이용하여 파일 로드\n",
        "            x = torchaudio.load(path)[0]\n",
        "            # Filtering\n",
        "            if self.filtering is True:\n",
        "                cutoff_frequency = self.filter_hz\n",
        "                x = self.apply_filter_torchaudio(x, cutoff_frequency)\n",
        "            ms = ta_transforms.MelSpectrogram(sample_rate=SAMPLE_RATE,\n",
        "                                        n_fft=128,\n",
        "                                        win_length=100,\n",
        "                                        n_mels=40,\n",
        "                                        hop_length=HOP_LENGTH)(x)\n",
        "\n",
        "            # 스펙트로그램의 형태를 유지하면서 torchaudio로 불러왔을 때와 같이 0~1로 정규화\n",
        "            # ms = self.normalize_spectrogram(x, ms)\n",
        "\n",
        "            # 원본 wav의 길이가 th보다 길다면 Slicing\n",
        "            if ms.shape[-1] > self.th:\n",
        "                scale = 1\n",
        "                num_splits = ms.shape[-1] // self.th    # wav길이 == th의 배수\n",
        "                if ms.shape[-1] % self.th != 0: # wav길이 != th의 배수\n",
        "                    num_splits += 1\n",
        "                self.iter_list.append(num_splits)\n",
        "\n",
        "                for i in range(num_splits):\n",
        "                    start_idx = i * self.th\n",
        "                    end_idx = (i + 1) * self.th\n",
        "                    split = ms[..., start_idx:end_idx]\n",
        "\n",
        "                    # th보다 길이가 짧다면\n",
        "                    if split.shape[-1] < self.th:\n",
        "                        # Resizing\n",
        "                        if self.resizing is True:\n",
        "                            scale = self.th / split.shape[-1]\n",
        "                            target_shape = (split.shape[-2], self.th)\n",
        "                            split = self.resize_spectrogram(split, target_shape)\n",
        "                        # Padding\n",
        "                        else:\n",
        "                            split = self.padding(split, self.th)\n",
        "                    # 최종 Resizing\n",
        "                    resized = self.resize_spectrogram(split, self.target_size)\n",
        "                    resized = self.normalize_spectrogram2(resized)  # 정규화\n",
        "                    audio_list.append(resized)\n",
        "                    if self.resizing is True:\n",
        "                        self.scale_list.append(scale)\n",
        "\n",
        "            # 원본 wav의 길이가 th보다 짧거나 같다면\n",
        "            else:\n",
        "                self.iter_list.append(1)\n",
        "                scale = 1\n",
        "                # th보다 짧다면\n",
        "                if ms.shape[-1] < self.th:\n",
        "                    # Resizing\n",
        "                    if self.resizing is True:\n",
        "                        scale = self.th / ms.shape[-1]\n",
        "                        target_shape = (ms.shape[-2], self.th)\n",
        "                        ms = self.resize_spectrogram(ms, target_shape)\n",
        "                    # Padding\n",
        "                    else:\n",
        "                        ms = self.padding(ms, self.th)\n",
        "                # 최종 resizing\n",
        "                ms = self.resize_spectrogram(ms, self.target_size)\n",
        "                ms = self.normalize_spectrogram2(ms)    # 정규화\n",
        "                audio_list.append(ms)\n",
        "                if self.resizing is True:\n",
        "                    self.scale_list.append(scale)\n",
        "        return torch.stack(audio_list)\n",
        "\n",
        "    def get_label(self):\n",
        "        labels = []\n",
        "        idx=0\n",
        "        for i, path_tsv in enumerate(self.tsvs):\n",
        "            path = os.path.join(PATH, path_tsv)\n",
        "            tsv_data = pd.read_csv(path, sep='\\t', header=None)\n",
        "            iter = self.iter_list[i]\n",
        "            continuous = False\n",
        "            next_end = 0\n",
        "            next_class = 0\n",
        "            for _iter in range(iter):\n",
        "                label = []\n",
        "                if self.resizing is True:\n",
        "                    scale = self.scale_list[sum(self.iter_list[:i]) + _iter]\n",
        "                for _, tsv_row in tsv_data.iterrows():\n",
        "                    # 이전 구간에서 이어진다면 tsv_row[0] = 0\n",
        "                    if continuous is True:\n",
        "                        tsv_row[0] = 0\n",
        "                        tsv_row[1] = next_end\n",
        "                        tsv_row[2] = next_class\n",
        "                        continuous = False\n",
        "                        # resize를 하였다면 라벨 값도 스케일링\n",
        "                        if self.resizing is True:\n",
        "                            tsv_row[0] *= scale\n",
        "                            tsv_row[1] *= scale\n",
        "\n",
        "                    # 이전 구간에서 이어지지 않는다면 새로 데이터 가져오기\n",
        "                    elif tsv_row[2] in [1, 3]:\n",
        "                        # 구간 불러와서 sr값 곱하고 hop_legth로 나누기\n",
        "                        tsv_row[0] = tsv_row[0] * SAMPLE_RATE / HOP_LENGTH - (_iter * self.th)\n",
        "                        tsv_row[1] = tsv_row[1] * SAMPLE_RATE / HOP_LENGTH - (_iter * self.th)\n",
        "                        tsv_row[2] = 0 if tsv_row[2] == 1 else 1    # S1=0, S2=1\n",
        "                        # 시작점이 th 이상이라면, 이전 구간 이미지의 라벨이라면 continue\n",
        "                        if tsv_row[0] >= self.th or tsv_row[0] < 0:\n",
        "                            continue\n",
        "                        # th보다 길이가 짧다면(나뉜 이미지의 가장 마지막 이미지라면)\n",
        "                        if _iter == iter - 1:\n",
        "                            # resize를 하였다면 라벨 값도 스케일링\n",
        "                            if self.resizing is True:\n",
        "                                tsv_row[0] *= scale\n",
        "                                tsv_row[1] *= scale\n",
        "\n",
        "                        # th보다 길이가 길다면 Slicing\n",
        "                        elif tsv_row[1] >= self.th:\n",
        "                            next_end = tsv_row[1] - self.th\n",
        "                            next_class = tsv_row[2]\n",
        "                            tsv_row[1] = self.th # 지움\n",
        "                            continuous = True\n",
        "                            # 최종 resize한 값 으로 보간\n",
        "                            tsv_row[0] *= self.target_size[1] / self.th\n",
        "                            tsv_row[1] *= self.target_size[1] / self.th\n",
        "\n",
        "                            # S1=1, S2=2\n",
        "                            label.append([tsv_row[0] / self.target_size[0], 0 / self.target_size[0],\n",
        "                                  tsv_row[1] / self.target_size[0], self.target_size[0] / self.target_size[0],\n",
        "                                  int(tsv_row[2])+1])# xmin, ymin, xmax, ymax, cls\n",
        "                            break\n",
        "\n",
        "                    # 이전 값에서 이어지지 않으면서 S1, S2가 아닌 경우 continue\n",
        "                    else: continue\n",
        "\n",
        "                    # 최종 resize한 값 으로 보간\n",
        "                    tsv_row[0] *= self.target_size[1] / self.th\n",
        "                    tsv_row[1] *= self.target_size[1] / self.th\n",
        "\n",
        "                    label.append([tsv_row[0] / self.target_size[0], 0 / self.target_size[0],\n",
        "                                  tsv_row[1] / self.target_size[0], self.target_size[0] / self.target_size[0],\n",
        "                                  int(tsv_row[2])+1])# xmin, ymin, xmax, ymax, cls\n",
        "\n",
        "                    #label.append((int(tsv_row[2]), tsv_row[0], tsv_row[1]))\n",
        "                if(len(label)==0):\n",
        "                    self.delete_list.append(idx)\n",
        "                idx+=1\n",
        "                labels.append(label)\n",
        "        return labels\n",
        "\n",
        "    def delete_data(self):\n",
        "        delete_count=0\n",
        "        for i in self.delete_list:\n",
        "            del self.y[i-delete_count]\n",
        "            delete_count+=1\n",
        "\n",
        "        self.x = self.x[[i for i in range(self.x.size(0)) if i not in self.delete_list]]\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(PATH, train_patient_txt, target_size=(300, 300), th=5, resizing=True, filtering=True)"
      ],
      "metadata": {
        "id": "wMMJglNN8cXG"
      },
      "id": "wMMJglNN8cXG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset.delete_list)\n",
        "dataset.delete_list\n",
        "print(len(dataset.y),len(dataset.x))\n",
        "dataset.x.size(0)"
      ],
      "metadata": {
        "id": "DXz6Bv_28eod"
      },
      "id": "DXz6Bv_28eod",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "520a6d1a",
      "metadata": {
        "id": "520a6d1a"
      },
      "source": [
        "# data loader 생성"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "\n",
        "    targets = []\n",
        "    imgs = []\n",
        "    for sample in batch:\n",
        "        imgs.append(sample[0])  # sample[0]은 화상 gt\n",
        "        targets.append(torch.FloatTensor(sample[1]))  # sample[1]은 어노테이션 gt\n",
        "\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "    return imgs, targets\n",
        "\n",
        "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=my_collate_fn)"
      ],
      "metadata": {
        "id": "8hqNaTdS8iAE"
      },
      "id": "8hqNaTdS8iAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6eb3aeb3",
      "metadata": {
        "id": "6eb3aeb3"
      },
      "source": [
        "# model 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6ed4fa69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ed4fa69",
        "outputId": "8614c135-3fc9-4ea7-e090-464336dc5839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n",
            "100%|██████████| 21.1M/21.1M [00:00<00:00, 76.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "backbone = models.mobilenet_v3_large(pretrained=True)\n",
        "\n",
        "#for param in backbone.parameters():\n",
        "#    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ed1c1f23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed1c1f23",
        "outputId": "51cb115e-c940-4ba3-d895-1a40bb0399c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features\n",
            "avgpool\n",
            "classifier\n",
            "Conv2dNormActivation(\n",
            "  (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "  (2): Hardswish()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "for name, module in backbone.named_children():\n",
        "    print(name)\n",
        "\n",
        "for n, x in enumerate(backbone.features.children()):\n",
        "    if n==0:\n",
        "        seq=x.children()\n",
        "        seq=next(seq)\n",
        "\n",
        "        prev_weight = seq.weight\n",
        "        new_weight = prev_weight[:, :1, :, :]\n",
        "        seq.weight = nn.Parameter(new_weight)\n",
        "        seq.in_channels = 1\n",
        "    if n==13:\n",
        "        seq=x.children()\n",
        "        seq=next(seq)\n",
        "        print(seq[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1942313a",
      "metadata": {
        "id": "1942313a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class SSD(nn.Module):\n",
        "    def __init__(self, backbone, n_class=3, default_box_n=[4,6,6,6,4,4], state = \"Train\"):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_layer = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "\n",
        "        self.n_class = n_class\n",
        "        self.default_box_n = default_box_n\n",
        "        self.state = state\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        #가중치 초기화 인자\n",
        "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 112, 1, 1))  # there are 512 channels in conv4_3_feats\n",
        "        nn.init.constant_(self.rescale_factors, 20)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        #backbone\n",
        "\n",
        "        self.backbone_layer = nn.Sequential(\n",
        "            backbone.features,\n",
        "        )\n",
        "\n",
        "        #extra layer\n",
        "        self.extra_layer_1 = self.extra_layers(960,512,4)\n",
        "        self.extra_layer_2 = self.extra_layers(512,256,4)\n",
        "        self.extra_layer_3 = self.extra_layers(256,256,2)\n",
        "        self.extra_layer_4 = self.extra_layers(256,128,2)\n",
        "\n",
        "        self.extra_layers = [self.extra_layer_1,\n",
        "                            self.extra_layer_2,\n",
        "                            self.extra_layer_3,\n",
        "                            self.extra_layer_4]\n",
        "\n",
        "\n",
        "        #detection output\n",
        "\n",
        "        self.cls_layers = nn.ModuleList([\n",
        "                                        nn.Conv2d(672, default_box_n[0]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                                        nn.Conv2d(960, default_box_n[1]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                                        nn.Conv2d(512, default_box_n[2]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                                        nn.Conv2d(256, default_box_n[3]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                                        nn.Conv2d(256, default_box_n[4]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                                        nn.Conv2d(128, default_box_n[5]*(self.n_class), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "                                        ])\n",
        "\n",
        "        self.loc_layers = nn.ModuleList([\n",
        "                                        nn.Conv2d(672, default_box_n[0]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                                        nn.Conv2d(960, default_box_n[1]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                                        nn.Conv2d(512, default_box_n[2]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                                        nn.Conv2d(256, default_box_n[3]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                                        nn.Conv2d(256, default_box_n[4]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                                        nn.Conv2d(128, default_box_n[5]*4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "                                        ])\n",
        "\n",
        "\n",
        "\n",
        "    def init_conv2d(self):\n",
        "        #가중치 초기화 함수\n",
        "\n",
        "        #모델학습이 순조롭지않다면 향후 추가 예정\n",
        "        return\n",
        "\n",
        "    def extra_layers(self, input_size, output_size, div):\n",
        "        layer = nn.Sequential(\n",
        "            #conv2D 해상도낮추기\n",
        "            nn.Conv2d(input_size, output_size, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(output_size, eps=0.001, momentum=0.01, affine=True, track_running_stats=True),\n",
        "            nn.Hardswish(),\n",
        "\n",
        "            #Inverted Residual (mobilev2 + squeeze)\n",
        "\n",
        "            #depthwise\n",
        "            # kernel size 5 고려해보기\n",
        "            nn.Conv2d(output_size, output_size, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=output_size, bias=False),\n",
        "            nn.BatchNorm2d(output_size, eps=0.001, momentum=0.01, affine=True, track_running_stats=True),\n",
        "            nn.Hardswish(),\n",
        "\n",
        "            #SqueezeExcitation\n",
        "            nn.Conv2d(output_size, output_size//div, kernel_size=(1, 1), stride=(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(output_size//div, output_size, kernel_size=(1, 1), stride=(1, 1)),\n",
        "\n",
        "            #Point-wise\n",
        "            nn.Conv2d(output_size, output_size, kernel_size=(1, 1), stride=(1, 1)),\n",
        "            nn.Hardswish(),\n",
        "            nn.Identity()\n",
        "\n",
        "        )\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_maps=[]\n",
        "        for n, layer in enumerate(backbone.features):#12 16\n",
        "            if n==13:\n",
        "                #L2 norm\n",
        "                #norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()     # (N, 1, 19, 19)\n",
        "                #featureMap_1 = x / norm                                  # (N, 112, 19, 19)\n",
        "                #featureMap_1= conv4_3 * self.rescale_factors            # (N, 512, 19, 19)\n",
        "\n",
        "                #13계층 bottleneck까지만\n",
        "                seq_layer = next(layer.children())\n",
        "                for seq_n in range(4):\n",
        "                    x = seq_layer[seq_n](x)\n",
        "                    if seq_n==0:\n",
        "                        f_maps.append(x)\n",
        "                continue\n",
        "\n",
        "            x = layer(x)\n",
        "            #print(\"size : {0}, number = {1}\".format(x.size(), n))\n",
        "            if n==16:\n",
        "                f_maps.append(x)\n",
        "\n",
        "        #extra layer\n",
        "        mini_batch_n = x.size(0)\n",
        "        for extra_layer in self.extra_layers:\n",
        "            for idx, layer in enumerate(extra_layer.children()):\n",
        "                if mini_batch_n ==1 and (idx == 1 or idx == 4):\n",
        "                    continue\n",
        "                x = layer(x)\n",
        "\n",
        "            f_maps.append(x)\n",
        "\n",
        "        cls = []\n",
        "        loc = []\n",
        "        for f_map, cls_layer, loc_layer in zip(f_maps,self. cls_layers, self.loc_layers):\n",
        "            output_cls = cls_layer(f_map)\n",
        "            output_loc = loc_layer(f_map)\n",
        "\n",
        "            cls.append(output_cls.permute(0, 2, 3, 1).contiguous())\n",
        "            loc.append(output_loc.permute(0, 2, 3, 1).contiguous())\n",
        "            #cls.append(output_cls)\n",
        "            #loc.append(output_loc)\n",
        "\n",
        "        cls = torch.cat([o.view(o.size(0), -1) for o in cls], 1)\n",
        "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
        "\n",
        "        loc = loc.view(loc.size(0), -1, 4)\n",
        "        if self.state == \"Train\":\n",
        "            #cls = cls.view(cls.size(0), -1, self.n_class)\n",
        "            cls = self.softmax(cls.view(cls.size(0), -1, self.n_class))\n",
        "        else:\n",
        "            cls = self.softmax(cls.view(cls.size(0), -1, self.n_class))\n",
        "\n",
        "\n",
        "        return cls, loc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cefcac5",
      "metadata": {
        "id": "4cefcac5"
      },
      "source": [
        "# default box, loss 함수 기타 기능 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7d8f4fa4",
      "metadata": {
        "id": "7d8f4fa4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def point_form(boxes):\n",
        "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n",
        "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n",
        "\n",
        "\n",
        "def center_size(boxes):\n",
        "    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n",
        "                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n",
        "\n",
        "\n",
        "def intersect(box_a, box_b):\n",
        "    A = box_a.size(0) #정답개수\n",
        "    B = box_b.size(0) #defalut 박스개수\n",
        "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2)) #정답, dbox , xmax, ymax\n",
        "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
        "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
        "    return inter[:, :, 0] * inter[:, :, 1]\n",
        "\n",
        "\n",
        "def jaccard(box_a, box_b):# iou\n",
        "    inter = intersect(box_a, box_b) # (target_n,dbox_n)\n",
        "    # 확인햇음\n",
        "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
        "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
        "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
        "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
        "    union = area_a + area_b - inter\n",
        "    return inter / union  # [A,B]\n",
        "\n",
        "\n",
        "def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n",
        "    #groud truth =target\n",
        "    # jaccard index\n",
        "    # defuatl box와 truths box iou 계산\n",
        "    overlaps = jaccard(\n",
        "        truths,\n",
        "        point_form(priors)\n",
        "    )\n",
        "\n",
        "    #overlaps.size() = target_n, dbox_n\n",
        "\n",
        "    # target기준 가장 성능이 좋은 dbox 값 검출, dbox 인덱스 검출\n",
        "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True) #차원, 형태 삭제 x size = target,1\n",
        "\n",
        "    # dbox 기준 가장 성능이 높았던 target 검출, target 인덱스 검출\n",
        "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True) #size = 1, bbox\n",
        "\n",
        "    #차원 제거\n",
        "    best_truth_idx.squeeze_(0)\n",
        "    best_truth_overlap.squeeze_(0)\n",
        "    best_prior_idx.squeeze_(1)\n",
        "    best_prior_overlap.squeeze_(1)\n",
        "    #print(best_truth_overlap, best_truth_idx)\n",
        "    #dbox기준 target값을 dbox 인덱스에 해당하면 값을 2로 변경\n",
        "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # 0차원에  best_prior_idx 위치에 value 2를 채움\n",
        "    #1, bbox     fill = target,1 ?????\n",
        "    for j in range(best_prior_idx.size(0)):\n",
        "        #타겟별 가장 성능이 좋은 dbox의 index 추출\n",
        "        #해당 index의 ddox의 가장 성능이 좋았던 타겟값으로 변경 - 안해도 되는작업이아닌가?\n",
        "        best_truth_idx[best_prior_idx[j]] = j\n",
        "\n",
        "    #print(truths.size())\n",
        "    matches = truths[best_truth_idx]          # Shape: [num_priors,4] #dbox 번호별로 가장 어울리는 타겟값의 정보를 저장함\n",
        "    conf = labels[best_truth_idx]#+1         # Shape: [num_priors] # dbox 번호별로 가장 어울리는 타겟값의 라벨을 저장함\n",
        "    conf[best_truth_overlap < threshold] = 0  # 실제 target값이더라도 dbox들의 iou 값이 낮다면 배경으로 판단\n",
        "\n",
        "    #해당코드에서 문제가 발생된다고 예상\n",
        "    loc = encode(matches, priors, variances, truths)\n",
        "\n",
        "    # idx 는 미니 배치 index\n",
        "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
        "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
        "\n",
        "\n",
        "def encode(matched, priors, variances, truths):\n",
        "    #torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    # dist b/t match center and prior's center\n",
        "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
        "    # encode variance\n",
        "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
        "\n",
        "    #print(\"****************************************************\")\n",
        "    eps = 1e-4\n",
        "    # match wh / prior wh\n",
        "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
        "    g_wh = torch.log(g_wh+eps) / variances[1]\n",
        "\n",
        "    #print(\"****************************************************\")\n",
        "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n",
        "\n",
        "\n",
        "# Adapted from https://github.com/Hakuyume/chainer-ssd\n",
        "def decode(loc, priors, variances):\n",
        "    boxes = torch.cat((\n",
        "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
        "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def log_sum_exp(x):\n",
        "    x_max = x.data.max()\n",
        "    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n",
        "\n",
        "\n",
        "def nms(boxes, scores, overlap=0.5, top_k=200):\n",
        "    keep = scores.new(scores.size(0)).zero_().long()\n",
        "    if boxes.numel() == 0:\n",
        "        return keep\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "    area = torch.mul(x2 - x1, y2 - y1)\n",
        "    v, idx = scores.sort(0)  # sort in ascending order\n",
        "    # I = I[v >= 0.01]\n",
        "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
        "    xx1 = boxes.new()\n",
        "    yy1 = boxes.new()\n",
        "    xx2 = boxes.new()\n",
        "    yy2 = boxes.new()\n",
        "    w = boxes.new()\n",
        "    h = boxes.new()\n",
        "\n",
        "    # keep = torch.Tensor()\n",
        "    count = 0\n",
        "    while idx.numel() > 0:\n",
        "        i = idx[-1]  # index of current largest val\n",
        "        # keep.append(i)\n",
        "        keep[count] = i\n",
        "        count += 1\n",
        "        if idx.size(0) == 1:\n",
        "            break\n",
        "        idx = idx[:-1]  # remove kept element from view\n",
        "        # load bboxes of next highest vals\n",
        "        torch.index_select(x1, 0, idx, out=xx1)\n",
        "        torch.index_select(y1, 0, idx, out=yy1)\n",
        "        torch.index_select(x2, 0, idx, out=xx2)\n",
        "        torch.index_select(y2, 0, idx, out=yy2)\n",
        "        # store element-wise max with next highest score\n",
        "        xx1 = torch.clamp(xx1, min=x1[i])\n",
        "        yy1 = torch.clamp(yy1, min=y1[i])\n",
        "        xx2 = torch.clamp(xx2, max=x2[i])\n",
        "        yy2 = torch.clamp(yy2, max=y2[i])\n",
        "        w.resize_as_(xx2)\n",
        "        h.resize_as_(yy2)\n",
        "        w = xx2 - xx1\n",
        "        h = yy2 - yy1\n",
        "        # check sizes of xx1 and xx2.. after each iteration\n",
        "        w = torch.clamp(w, min=0.0)\n",
        "        h = torch.clamp(h, min=0.0)\n",
        "        inter = w*h\n",
        "        # IoU = i / (area(a) + area(b) - i)\n",
        "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
        "        union = (rem_areas - inter) + area[i]\n",
        "        IoU = inter/union  # store result in iou\n",
        "        # keep only elements with an IoU <= overlap\n",
        "        idx = idx[IoU.le(overlap)]\n",
        "    return keep, count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "468dfba9",
      "metadata": {
        "id": "468dfba9"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiBoxLoss(nn.Module):\n",
        "    \"\"\"SSD의 손실함수 클래스 \"\"\"\n",
        "\n",
        "    def __init__(self, thresh=0.5, neg_pos=3, device='cpu'):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.jaccard_thresh = thresh  # 0.5 match 함수의 jaccard 계수의 임계치\n",
        "        self.negpos_ratio = neg_pos  # 3:1 Hard Negative Mining의 음과 양 비율\n",
        "        self.device = device  # 계산 device(CPU | GPU)\n",
        "\n",
        "    def forward(self, predictions, targets, dboxs):\n",
        "        \"\"\"\n",
        "        파라미터 설명\n",
        "        ----------\n",
        "        predictions :모델의 예측값 cls와 loc\n",
        "        cls는 batch dbox의 개수, 클래스 개수로 이루어짐\n",
        "        loc은 batch dbox의 개수, 4\n",
        "\n",
        "        targets : [num_batch, 객체개수, 5]\n",
        "            5는 라벨 정보[xmin, ymin, xmax, ymax, label_ind]\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        conf_data, loc_data = predictions\n",
        "        dbox_list = dboxs\n",
        "\n",
        "        num_batch = loc_data.size(0)  # 배치 크기\n",
        "        num_dbox = loc_data.size(1)  # DBox의 수\n",
        "        num_classes = conf_data.size(2)  # 클래스 수\n",
        "\n",
        "        # 손실 계산에 사용할 것을 저장하는 변수 작성\n",
        "        # conf_t_label：각 DBox에 가장 가까운 정답 BBox의 라벨을 저장\n",
        "        # loc_t: 각 DBox에 가장 가까운 정답 BBox의 위치 정보 저장\n",
        "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
        "\n",
        "        #conf_t_label.fill_(0)#테스트용도\n",
        "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
        "\n",
        "        # loc_t와 conf_t_label에\n",
        "        # DBox와 정답 어노테이션 targets를 amtch한 결과 덮어쓰기\n",
        "        for idx in range(num_batch):  # 미니 배치 루프\n",
        "\n",
        "            truths = targets[idx][:, :-1].to(self.device)\n",
        "            labels = targets[idx][:, -1].to(self.device)\n",
        "            dbox = dbox_list.to(self.device)\n",
        "\n",
        "            # match 함수를 실행하여 loc_t와 conf_t_label 내용 갱신\n",
        "            # loc_t: 각 DBox에 가장 가까운 정답 BBox 위치 정보가 덮어써짐.\n",
        "            # conf_t_label：각 DBox에 가장 가까운 정답 BBox 라벨이 덮어써짐.\n",
        "            # 단, 가장 가까운 BBox와 iou가 0.5보다 작은 경우,\n",
        "            # 정답 BBox의 라벨 conf_t_label은 배경 클래스 0으로 한다.\n",
        "            variance = [0.1, 0.2]\n",
        "\n",
        "            # 라벨을 dbox에 대한 offset으로 변환\n",
        "            match(self.jaccard_thresh, truths, dbox,\n",
        "                  variance, labels, loc_t, conf_t_label, idx)\n",
        "\n",
        "\n",
        "        #물체를 발견한 offset만 손실 계산\n",
        "        pos_mask = conf_t_label > 0  # size: batch,dbox,1\n",
        "        true_count = pos_mask[0].sum().item()\n",
        "\n",
        "        # pos_mask를 loc_data 크기로 변형\n",
        "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
        "\n",
        "        # Positive DBox의 loc_data와 offset loc_t 취득\n",
        "        loc_p = loc_data[pos_idx].view(-1, 4).to(device)\n",
        "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
        "\n",
        "        # 물체를 발견한 Positive DBox의 오프셋 정보 loc_t의 손실(오차)를 계산\n",
        "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
        "        # ----------\n",
        "        # 클래스 예측의 손실 : loss_c를 계산\n",
        "        # 교차 엔트로피 오차 함수로 손실 계산. 단 배경 클래스가 정답인 DBox가 압도적으로 많으므로,\n",
        "        # Hard Negative Mining을 실시하여 물체 발견 DBox 및 배경 클래스 DBox의 비율이 1:3이 되도록 한다.\n",
        "        # 배경 클래스 DBox로 예상한 것 중 손실이 적은 것은 클래스 예측 손실에서 제외\n",
        "        # ----------\n",
        "        batch_conf = conf_data.view(-1, num_classes)\n",
        "\n",
        "        # 클래스 예측의 손실함수 계산(reduction='none'으로 하여 합을 취하지 않고 차원 보존)\n",
        "        loss_c = F.cross_entropy(\n",
        "            batch_conf, conf_t_label.view(-1), reduction='none')#batch * dbox_n\n",
        "\n",
        "\n",
        "        #------------이 아래 부분은 이해 못함\n",
        "\n",
        "\n",
        "\n",
        "        # -----------------\n",
        "        # Negative DBox중 Hard Negative Mining으로\n",
        "        # 추출하는 것을 구하는 마스크 작성\n",
        "        # -----------------\n",
        "\n",
        "        # 물체를 발견한 Positive DBox의 손실을 0으로 한다.\n",
        "        # (주의) 물체는 label이 1 이상, 라벨 0은 배경을 의미\n",
        "        num_pos = pos_mask.long().sum(1, keepdim=True)  # 미니 배치별 물체 클래스 예측 수\n",
        "        loss_c = loss_c.view(num_batch, -1)  # torch.Size([num_batch, 8732])\n",
        "        loss_c[pos_mask] = 0  # 물체를 발견한 DBox는 손실 0으로 한다.\n",
        "\n",
        "        # Hard Negative Mining\n",
        "        # 각 DBox 손실의 크기 loss_c 순위 idx_rank를 구함\n",
        "        _, loss_idx = loss_c.sort(1, descending=True)\n",
        "        _, idx_rank = loss_idx.sort(1)\n",
        "\n",
        "        #  (주의) 구현된 코드는 특수하며 직관적이지 않음.\n",
        "        # 위 두 줄의 요점은 각 DBox에 대해 손실 크기가 몇 번째인지의 정보를\n",
        "        # idx_rank 변수로 빠르게 얻는 코드이다.\n",
        "\n",
        "        # DBox의 손실 값이 큰 쪽부터 내림차순으로 정렬하여,\n",
        "        # DBox의 내림차순의 index를 loss_idx에 저장한다.\n",
        "        # 손실 크기 loss_c의 순위 idx_rank를 구한다.\n",
        "        # 내림차순이 된 배열 index인 loss_idx를 0부터 8732까지 오름차순으로 다시 정렬하기 위하여\n",
        "        # 몇 번째 loss_idx의 인덱스를 취할 지 나타내는 것이 idx_rank이다.\n",
        "        # 예를 들면 idx_rank 요소의 0번째 = idx_rank[0]을 구하는 것은 loss_idx의 값이 0인 요소,\n",
        "        # 즉 loss_idx[?] =0은 원래 loss_c의 요소 0번째는 내림차순으로 정렬된 loss_idx의\n",
        "        # 몇 번째입니까? 를구하는 것이 되어 결과적으로,\n",
        "        # ? = idx_rank[0]은 loss_c의 요소 0번째가 내림차순으로 몇 번째인지 나타냄\n",
        "\n",
        "        # 배경 DBox의 수 num_neg를 구한다. HardNegative Mining으로\n",
        "        # 물체 발견 DBox으 ㅣ수 num_pos의 세 배 (self.negpos_ratio 배)로 한다.\n",
        "        # DBox의 수를 초과한 경우에는 DBox의 수를 상한으로 한다.\n",
        "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\n",
        "\n",
        "        # idx_rank에 각 DBox의 손실 크기가 위에서부터 몇 번째인지 저장되었다.\n",
        "        # 배경 DBox의 수 num_neg보다 순위가 낮은(손실이 큰) DBox를 취하는 마스크 작성\n",
        "        # torch.Size([num_batch, 8732])\n",
        "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
        "\n",
        "        # -----------------\n",
        "        # (종료) 지금부터 Negative DBox 중 Hard Negative Mining으로 추출할 것을 구하는 마스크를 작성\n",
        "        # -----------------\n",
        "\n",
        "        # 마스크 모양을 고쳐 conf_data에 맞춘다\n",
        "        # pos_idx_mask는 Positive DBox의 conf를 꺼내는 마스크이다.\n",
        "        # neg_idx_mask는 Hard Negative Mining으로 추출한 Negative DBox의 conf를 꺼내는 마스크이다.\n",
        "        # pos_mask：torch.Size([num_batch, 8732])\n",
        "        # --> pos_idx_mask：torch.Size([num_batch, 8732, 21])\n",
        "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
        "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
        "\n",
        "        # conf_data에서 pos와 neg만 꺼내서 conf_hnm으로 한다.\n",
        "        # 형태는 torch.Size([num_pos+num_neg, 21])\n",
        "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)\n",
        "                             ].view(-1, num_classes)\n",
        "        # gt는 greater than (>)의 약칭. mask가 1인 index를 꺼낸다\n",
        "        # pos_idx_mask+neg_idx_mask는 덧셈이지만 index로 mask를 정리할 뿐임.\n",
        "        # pos이든 neg이든 마스크가 1인 것을 더해 하나의 리스트로 만들어 이를 gt로 췯그한다.\n",
        "\n",
        "        # 마찬가지로 지도 데이터인 conf_t_label에서 pos와 neg만 꺼내, conf_t_label_hnm 으로\n",
        "        # torch.Size([pos+neg]) 형태가 된다\n",
        "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
        "\n",
        "        # confidence의 손실함수 계산(요소의 합계=sum을 구함)\n",
        "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\n",
        "\n",
        "        # 물체를 발견한 BBox의 수 N (전체 미니 배치의 합계) 으로 손실을 나눈다.\n",
        "        N = num_pos.sum()\n",
        "        loss_l /= N\n",
        "        loss_c /= N\n",
        "\n",
        "        #print(\"-\"*100)\n",
        "\n",
        "        return loss_l, loss_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ff880acd",
      "metadata": {
        "id": "ff880acd"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from math import sqrt\n",
        "from itertools import product as product\n",
        "\n",
        "class default:\n",
        "\n",
        "    def __init__(self, image_size=300, feature_maps=[19, 10, 5, 3, 2, 1], min_sizes=[]):\n",
        "        super(default, self).__init__()\n",
        "        self.image_size = 300\n",
        "        # number of priors for feature map location (either 4 or 6)\n",
        "        self.feature_maps = feature_maps\n",
        "        self.min_sizes = [16, 30, 60, 100, 150, 300]  #0.2, 0.34, 0.48, 0.62, 0.76, 0.9?\n",
        "        self.max_sizes = [30, 60, 100, 150, 300, 300]\n",
        "        self.steps = [19,10,5,3,2,1] # 이미지 그리드로 나눈 개수\n",
        "        self.aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
        "        self.clip = True\n",
        "\n",
        "    def forward(self):\n",
        "        mean = []\n",
        "        for k, f in enumerate(self.feature_maps):\n",
        "            for i, j in product(range(f), repeat=2):\n",
        "                f_k = self.steps[k] # 그리드 개수\n",
        "                # default 박스 중점\n",
        "                cx = (j + 0.5) / f_k\n",
        "                cy = (i + 0.5) / f_k\n",
        "                # aspect_ratio: 1\n",
        "                # default 박스 공식이 아닌 임의로 설정 그리드 크기를 기준으로 나눔\n",
        "                s_k = self.min_sizes[k]/self.image_size\n",
        "                mean += [cx, cy, s_k, 0.7] # s_k\n",
        "\n",
        "                # aspect_ratio: 1\n",
        "                # s_k*s_k+1\n",
        "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
        "\n",
        "                #print(cx,cy,s_k_prime,self.max_sizes[k]/self.image_size)\n",
        "                mean += [cx, cy, s_k_prime, 0.7] #s_k\n",
        "\n",
        "                # rest of aspect ratios\n",
        "                for ar in self.aspect_ratios[k]:\n",
        "                    mean += [cx, cy, s_k*sqrt(ar), 0.7] # s_k/sqrt(ar)\n",
        "                    mean += [cx, cy, s_k/sqrt(ar), 0.7]\n",
        "        # back to torch land\n",
        "        output = torch.Tensor(mean).view(-1, 4)\n",
        "        if self.clip:\n",
        "            output.clamp_(max=1, min=0)\n",
        "        return output\n",
        "    def cxcy_to_xy(self, tensor_default_box):\n",
        "        return tensor_default_box*self.image_size\n",
        "\n",
        "    def xy_to_cxcy(self, tensor_default_box):\n",
        "        return tensor_default_box/self.image_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed55e670",
      "metadata": {
        "id": "ed55e670",
        "outputId": "40183eaf-a201-4225-a9ac-ae5838849f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features\n",
            "avgpool\n",
            "classifier\n",
            "ConvBNActivation(\n",
            "  (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "  (2): Hardswish()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "backbone = models.mobilenet_v3_large(pretrained=True)\n",
        "for name, module in backbone.named_children():\n",
        "    print(name)\n",
        "\n",
        "for n, x in enumerate(backbone.features.children()):\n",
        "    if n==0:\n",
        "        seq=x.children()\n",
        "        seq=next(seq)\n",
        "\n",
        "        prev_weight = seq.weight\n",
        "        new_weight = prev_weight[:, :1, :, :]\n",
        "        seq.weight = nn.Parameter(new_weight)\n",
        "        seq.in_channels = 1\n",
        "    if n==13:\n",
        "        seq=x.children()\n",
        "        seq=next(seq)\n",
        "        print(seq[0])\n",
        "model = SSD(backbone, n_class=3)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a48da96a",
      "metadata": {
        "id": "a48da96a",
        "outputId": "35221f81-29e8-4846-d5e0-14191740e074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 1 / 10\n",
            "Current Batch 0 / 132 | Cls Loss : 6.894, Loc Loss : 5.200, Total Loss : 12.094 | 10 iter time 0.3998: \n",
            "Current Batch 10 / 132 | Cls Loss : 4.164, Loc Loss : 4.701, Total Loss : 8.865 | 10 iter time 0.3167: \n",
            "Current Batch 20 / 132 | Cls Loss : 2.876, Loc Loss : 4.646, Total Loss : 7.522 | 10 iter time 0.3187: \n",
            "Current Batch 30 / 132 | Cls Loss : 3.020, Loc Loss : 4.579, Total Loss : 7.598 | 10 iter time 0.3185: \n",
            "Current Batch 40 / 132 | Cls Loss : 2.905, Loc Loss : 4.521, Total Loss : 7.427 | 10 iter time 0.3113: \n",
            "Current Batch 50 / 132 | Cls Loss : 2.624, Loc Loss : 4.488, Total Loss : 7.112 | 10 iter time 0.3453: \n",
            "Current Batch 60 / 132 | Cls Loss : 2.897, Loc Loss : 4.480, Total Loss : 7.377 | 10 iter time 0.3114: \n",
            "Current Batch 70 / 132 | Cls Loss : 2.678, Loc Loss : 4.466, Total Loss : 7.144 | 10 iter time 0.3124: \n",
            "Current Batch 80 / 132 | Cls Loss : 2.659, Loc Loss : 4.464, Total Loss : 7.122 | 10 iter time 0.3190: \n",
            "Current Batch 90 / 132 | Cls Loss : 3.245, Loc Loss : 4.449, Total Loss : 7.693 | 10 iter time 0.3172: \n",
            "Current Batch 100 / 132 | Cls Loss : 3.177, Loc Loss : 4.478, Total Loss : 7.655 | 10 iter time 0.3160: \n",
            "Current Batch 110 / 132 | Cls Loss : 2.550, Loc Loss : 4.444, Total Loss : 6.994 | 10 iter time 0.3157: \n",
            "Current Batch 120 / 132 | Cls Loss : 2.606, Loc Loss : 4.471, Total Loss : 7.077 | 10 iter time 0.3228: \n",
            "Current Batch 130 / 132 | Cls Loss : 2.237, Loc Loss : 4.442, Total Loss : 6.678 | 10 iter time 0.3136: \n",
            "Epoch : 1 / 10 of Total Loss : Total Loss : 1013.171\n",
            "-----------------------------------------------\n",
            "Epoch : 2 / 10\n",
            "Current Batch 0 / 132 | Cls Loss : 2.561, Loc Loss : 4.458, Total Loss : 7.019 | 10 iter time 0.3217: \n",
            "Current Batch 10 / 132 | Cls Loss : 3.117, Loc Loss : 4.471, Total Loss : 7.588 | 10 iter time 0.3215: \n",
            "Current Batch 20 / 132 | Cls Loss : 2.125, Loc Loss : 4.422, Total Loss : 6.547 | 10 iter time 0.3231: \n",
            "Current Batch 30 / 132 | Cls Loss : 2.432, Loc Loss : 4.410, Total Loss : 6.842 | 10 iter time 0.3165: \n",
            "Current Batch 40 / 132 | Cls Loss : 2.447, Loc Loss : 4.429, Total Loss : 6.875 | 10 iter time 0.3100: \n",
            "Current Batch 50 / 132 | Cls Loss : 2.549, Loc Loss : 4.445, Total Loss : 6.993 | 10 iter time 0.3419: \n",
            "Current Batch 60 / 132 | Cls Loss : 2.400, Loc Loss : 4.427, Total Loss : 6.827 | 10 iter time 0.3215: \n",
            "Current Batch 70 / 132 | Cls Loss : 2.555, Loc Loss : 4.430, Total Loss : 6.985 | 10 iter time 0.3194: \n",
            "Current Batch 80 / 132 | Cls Loss : 2.553, Loc Loss : 4.418, Total Loss : 6.970 | 10 iter time 0.3293: \n",
            "Current Batch 90 / 132 | Cls Loss : 2.413, Loc Loss : 4.410, Total Loss : 6.823 | 10 iter time 0.3230: \n",
            "Current Batch 100 / 132 | Cls Loss : 2.522, Loc Loss : 4.409, Total Loss : 6.932 | 10 iter time 0.3217: \n",
            "Current Batch 110 / 132 | Cls Loss : 2.425, Loc Loss : 4.437, Total Loss : 6.861 | 10 iter time 0.3109: \n",
            "Current Batch 120 / 132 | Cls Loss : 2.092, Loc Loss : 4.433, Total Loss : 6.525 | 10 iter time 0.3126: \n",
            "Current Batch 130 / 132 | Cls Loss : 2.423, Loc Loss : 4.424, Total Loss : 6.847 | 10 iter time 0.3172: \n",
            "Epoch : 2 / 10 of Total Loss : Total Loss : 927.610\n",
            "-----------------------------------------------\n",
            "Epoch : 3 / 10\n",
            "Current Batch 0 / 132 | Cls Loss : 2.769, Loc Loss : 4.418, Total Loss : 7.188 | 10 iter time 0.3472: \n",
            "Current Batch 10 / 132 | Cls Loss : 2.719, Loc Loss : 4.449, Total Loss : 7.168 | 10 iter time 0.3157: \n",
            "Current Batch 20 / 132 | Cls Loss : 2.778, Loc Loss : 4.419, Total Loss : 7.197 | 10 iter time 0.3327: \n",
            "Current Batch 30 / 132 | Cls Loss : 1.971, Loc Loss : 4.414, Total Loss : 6.385 | 10 iter time 0.3410: \n",
            "Current Batch 40 / 132 | Cls Loss : 2.403, Loc Loss : 4.403, Total Loss : 6.806 | 10 iter time 0.3233: \n",
            "Current Batch 50 / 132 | Cls Loss : 2.210, Loc Loss : 4.410, Total Loss : 6.621 | 10 iter time 0.3148: \n",
            "Current Batch 60 / 132 | Cls Loss : 2.607, Loc Loss : 4.424, Total Loss : 7.031 | 10 iter time 0.3190: \n",
            "Current Batch 70 / 132 | Cls Loss : 2.447, Loc Loss : 4.431, Total Loss : 6.878 | 10 iter time 0.3072: \n",
            "Current Batch 80 / 132 | Cls Loss : 2.319, Loc Loss : 4.413, Total Loss : 6.732 | 10 iter time 0.3157: \n",
            "Current Batch 90 / 132 | Cls Loss : 2.154, Loc Loss : 4.417, Total Loss : 6.571 | 10 iter time 0.3063: \n",
            "Current Batch 100 / 132 | Cls Loss : 2.097, Loc Loss : 4.416, Total Loss : 6.513 | 10 iter time 0.3133: \n",
            "Current Batch 110 / 132 | Cls Loss : 1.972, Loc Loss : 4.401, Total Loss : 6.373 | 10 iter time 0.3152: \n",
            "Current Batch 120 / 132 | Cls Loss : 2.298, Loc Loss : 4.403, Total Loss : 6.701 | 10 iter time 0.3159: \n",
            "Current Batch 130 / 132 | Cls Loss : 2.551, Loc Loss : 4.400, Total Loss : 6.951 | 10 iter time 0.3376: \n",
            "Epoch : 3 / 10 of Total Loss : Total Loss : 904.913\n",
            "-----------------------------------------------\n",
            "Epoch : 4 / 10\n",
            "Current Batch 0 / 132 | Cls Loss : 2.182, Loc Loss : 4.384, Total Loss : 6.566 | 10 iter time 0.3506: \n",
            "Current Batch 10 / 132 | Cls Loss : 2.454, Loc Loss : 4.393, Total Loss : 6.847 | 10 iter time 0.3415: \n",
            "Current Batch 20 / 132 | Cls Loss : 3.103, Loc Loss : 4.429, Total Loss : 7.531 | 10 iter time 0.3220: \n",
            "Current Batch 30 / 132 | Cls Loss : 2.884, Loc Loss : 4.378, Total Loss : 7.262 | 10 iter time 0.3125: \n",
            "Current Batch 40 / 132 | Cls Loss : 1.926, Loc Loss : 4.393, Total Loss : 6.319 | 10 iter time 0.3235: \n",
            "Current Batch 50 / 132 | Cls Loss : 2.267, Loc Loss : 4.392, Total Loss : 6.659 | 10 iter time 0.3134: \n",
            "Current Batch 60 / 132 | Cls Loss : 2.197, Loc Loss : 4.399, Total Loss : 6.596 | 10 iter time 0.3217: \n",
            "Current Batch 70 / 132 | Cls Loss : 2.616, Loc Loss : 4.403, Total Loss : 7.019 | 10 iter time 0.3162: \n",
            "Current Batch 80 / 132 | Cls Loss : 2.247, Loc Loss : 4.403, Total Loss : 6.650 | 10 iter time 0.3141: \n",
            "Current Batch 90 / 132 | Cls Loss : 2.607, Loc Loss : 4.429, Total Loss : 7.036 | 10 iter time 0.3138: \n",
            "Current Batch 100 / 132 | Cls Loss : 1.872, Loc Loss : 4.440, Total Loss : 6.313 | 10 iter time 0.3127: \n",
            "Current Batch 110 / 132 | Cls Loss : 2.390, Loc Loss : 4.403, Total Loss : 6.793 | 10 iter time 0.3150: \n",
            "Current Batch 120 / 132 | Cls Loss : 2.038, Loc Loss : 4.414, Total Loss : 6.451 | 10 iter time 0.3229: \n",
            "Current Batch 130 / 132 | Cls Loss : 1.884, Loc Loss : 4.393, Total Loss : 6.277 | 10 iter time 0.3112: \n",
            "Epoch : 4 / 10 of Total Loss : Total Loss : 893.083\n",
            "-----------------------------------------------\n",
            "Epoch : 5 / 10\n",
            "Current Batch 0 / 132 | Cls Loss : 2.254, Loc Loss : 4.385, Total Loss : 6.639 | 10 iter time 0.3187: \n",
            "Current Batch 10 / 132 | Cls Loss : 2.211, Loc Loss : 4.397, Total Loss : 6.608 | 10 iter time 0.3199: \n",
            "Current Batch 20 / 132 | Cls Loss : 1.913, Loc Loss : 4.403, Total Loss : 6.317 | 10 iter time 0.3147: \n",
            "Current Batch 30 / 132 | Cls Loss : 2.487, Loc Loss : 4.414, Total Loss : 6.900 | 10 iter time 0.3188: \n",
            "Current Batch 40 / 132 | Cls Loss : 2.533, Loc Loss : 4.394, Total Loss : 6.927 | 10 iter time 0.3357: \n",
            "Current Batch 50 / 132 | Cls Loss : 2.300, Loc Loss : 4.382, Total Loss : 6.682 | 10 iter time 0.3200: \n",
            "Current Batch 60 / 132 | Cls Loss : 1.631, Loc Loss : 4.386, Total Loss : 6.017 | 10 iter time 0.3128: \n",
            "Current Batch 70 / 132 | Cls Loss : 2.396, Loc Loss : 4.407, Total Loss : 6.803 | 10 iter time 0.3115: \n",
            "Current Batch 80 / 132 | Cls Loss : 2.089, Loc Loss : 4.416, Total Loss : 6.505 | 10 iter time 0.3261: \n",
            "Current Batch 90 / 132 | Cls Loss : 2.223, Loc Loss : 4.393, Total Loss : 6.615 | 10 iter time 0.3164: \n",
            "Current Batch 100 / 132 | Cls Loss : 1.801, Loc Loss : 4.398, Total Loss : 6.199 | 10 iter time 0.3105: \n",
            "Current Batch 110 / 132 | Cls Loss : 2.111, Loc Loss : 4.400, Total Loss : 6.511 | 10 iter time 0.3239: \n",
            "Current Batch 120 / 132 | Cls Loss : 2.793, Loc Loss : 4.382, Total Loss : 7.175 | 10 iter time 0.3285: \n",
            "Current Batch 130 / 132 | Cls Loss : 2.122, Loc Loss : 4.397, Total Loss : 6.519 | 10 iter time 0.3175: \n",
            "Epoch : 5 / 10 of Total Loss : Total Loss : 886.286\n",
            "-----------------------------------------------\n",
            "Epoch : 6 / 10\n",
            "Current Batch 0 / 132 | Cls Loss : 1.956, Loc Loss : 4.379, Total Loss : 6.336 | 10 iter time 0.3600: \n",
            "Current Batch 10 / 132 | Cls Loss : 2.318, Loc Loss : 4.393, Total Loss : 6.711 | 10 iter time 0.3197: \n",
            "Current Batch 20 / 132 | Cls Loss : 2.453, Loc Loss : 4.359, Total Loss : 6.813 | 10 iter time 0.3274: \n",
            "Current Batch 30 / 132 | Cls Loss : 2.018, Loc Loss : 4.389, Total Loss : 6.407 | 10 iter time 0.3204: \n",
            "Current Batch 40 / 132 | Cls Loss : 2.793, Loc Loss : 4.406, Total Loss : 7.198 | 10 iter time 0.3161: \n",
            "Current Batch 50 / 132 | Cls Loss : 1.891, Loc Loss : 4.369, Total Loss : 6.260 | 10 iter time 0.3221: \n",
            "Current Batch 60 / 132 | Cls Loss : 2.288, Loc Loss : 4.385, Total Loss : 6.673 | 10 iter time 0.3240: \n",
            "Current Batch 70 / 132 | Cls Loss : 2.136, Loc Loss : 4.406, Total Loss : 6.542 | 10 iter time 0.3270: \n",
            "Current Batch 80 / 132 | Cls Loss : 1.915, Loc Loss : 4.357, Total Loss : 6.272 | 10 iter time 0.3347: \n",
            "Current Batch 90 / 132 | Cls Loss : 2.504, Loc Loss : 4.390, Total Loss : 6.895 | 10 iter time 0.3139: \n",
            "Current Batch 100 / 132 | Cls Loss : 1.966, Loc Loss : 4.400, Total Loss : 6.366 | 10 iter time 0.3320: \n",
            "Current Batch 110 / 132 | Cls Loss : 1.903, Loc Loss : 4.375, Total Loss : 6.278 | 10 iter time 0.3288: \n",
            "Current Batch 120 / 132 | Cls Loss : 2.350, Loc Loss : 4.364, Total Loss : 6.713 | 10 iter time 0.3273: \n",
            "Current Batch 130 / 132 | Cls Loss : 2.067, Loc Loss : 4.399, Total Loss : 6.466 | 10 iter time 0.3452: \n",
            "Epoch : 6 / 10 of Total Loss : Total Loss : 877.469\n",
            "-----------------------------------------------\n",
            "Epoch : 7 / 10\n",
            "Current Batch 0 / 132 | Cls Loss : 2.659, Loc Loss : 4.360, Total Loss : 7.019 | 10 iter time 0.3430: \n",
            "Current Batch 10 / 132 | Cls Loss : 1.975, Loc Loss : 4.368, Total Loss : 6.343 | 10 iter time 0.3298: \n",
            "Current Batch 20 / 132 | Cls Loss : 2.502, Loc Loss : 4.382, Total Loss : 6.884 | 10 iter time 0.3229: \n",
            "Current Batch 30 / 132 | Cls Loss : 2.132, Loc Loss : 4.362, Total Loss : 6.494 | 10 iter time 0.3228: \n",
            "Current Batch 40 / 132 | Cls Loss : 2.020, Loc Loss : 4.369, Total Loss : 6.389 | 10 iter time 0.3382: \n",
            "Current Batch 50 / 132 | Cls Loss : 2.168, Loc Loss : 4.380, Total Loss : 6.548 | 10 iter time 0.3286: \n",
            "Current Batch 60 / 132 | Cls Loss : 2.008, Loc Loss : 4.377, Total Loss : 6.385 | 10 iter time 0.3341: \n",
            "Current Batch 70 / 132 | Cls Loss : 2.173, Loc Loss : 4.359, Total Loss : 6.533 | 10 iter time 0.3292: \n",
            "Current Batch 80 / 132 | Cls Loss : 1.950, Loc Loss : 4.376, Total Loss : 6.326 | 10 iter time 0.3244: \n",
            "Current Batch 90 / 132 | Cls Loss : 1.828, Loc Loss : 4.369, Total Loss : 6.196 | 10 iter time 0.3313: \n",
            "Current Batch 100 / 132 | Cls Loss : 2.165, Loc Loss : 4.392, Total Loss : 6.557 | 10 iter time 0.3167: \n",
            "Current Batch 110 / 132 | Cls Loss : 1.774, Loc Loss : 4.359, Total Loss : 6.133 | 10 iter time 0.3205: \n",
            "Current Batch 120 / 132 | Cls Loss : 2.125, Loc Loss : 4.372, Total Loss : 6.497 | 10 iter time 0.3286: \n",
            "Current Batch 130 / 132 | Cls Loss : 2.716, Loc Loss : 4.384, Total Loss : 7.099 | 10 iter time 0.3391: \n",
            "Epoch : 7 / 10 of Total Loss : Total Loss : 871.749\n",
            "-----------------------------------------------\n",
            "Epoch : 8 / 10\n",
            "Current Batch 0 / 132 | Cls Loss : 1.794, Loc Loss : 4.382, Total Loss : 6.177 | 10 iter time 0.3528: \n",
            "Current Batch 10 / 132 | Cls Loss : 2.004, Loc Loss : 4.362, Total Loss : 6.366 | 10 iter time 0.3178: \n",
            "Current Batch 20 / 132 | Cls Loss : 2.188, Loc Loss : 4.385, Total Loss : 6.573 | 10 iter time 0.3421: \n",
            "Current Batch 30 / 132 | Cls Loss : 2.428, Loc Loss : 4.335, Total Loss : 6.764 | 10 iter time 0.3259: \n",
            "Current Batch 40 / 132 | Cls Loss : 1.860, Loc Loss : 4.359, Total Loss : 6.219 | 10 iter time 0.3263: \n",
            "Current Batch 50 / 132 | Cls Loss : 1.906, Loc Loss : 4.356, Total Loss : 6.261 | 10 iter time 0.3192: \n",
            "Current Batch 60 / 132 | Cls Loss : 2.284, Loc Loss : 4.379, Total Loss : 6.663 | 10 iter time 0.3267: \n",
            "Current Batch 70 / 132 | Cls Loss : 1.710, Loc Loss : 4.337, Total Loss : 6.047 | 10 iter time 0.3258: \n",
            "Current Batch 80 / 132 | Cls Loss : 2.464, Loc Loss : 4.390, Total Loss : 6.854 | 10 iter time 0.3243: \n",
            "Current Batch 90 / 132 | Cls Loss : 1.917, Loc Loss : 4.381, Total Loss : 6.298 | 10 iter time 0.3172: \n",
            "Current Batch 100 / 132 | Cls Loss : 2.488, Loc Loss : 4.365, Total Loss : 6.853 | 10 iter time 0.3482: \n",
            "Current Batch 110 / 132 | Cls Loss : 2.406, Loc Loss : 4.387, Total Loss : 6.793 | 10 iter time 0.3352: \n",
            "Current Batch 120 / 132 | Cls Loss : 1.927, Loc Loss : 4.360, Total Loss : 6.287 | 10 iter time 0.3168: \n",
            "Current Batch 130 / 132 | Cls Loss : 1.877, Loc Loss : 4.363, Total Loss : 6.241 | 10 iter time 0.3293: \n",
            "Epoch : 8 / 10 of Total Loss : Total Loss : 864.474\n",
            "-----------------------------------------------\n",
            "Epoch : 9 / 10\n",
            "Current Batch 0 / 132 | Cls Loss : 2.119, Loc Loss : 4.391, Total Loss : 6.511 | 10 iter time 0.3268: \n",
            "Current Batch 10 / 132 | Cls Loss : 2.136, Loc Loss : 4.386, Total Loss : 6.522 | 10 iter time 0.3377: \n",
            "Current Batch 20 / 132 | Cls Loss : 2.762, Loc Loss : 4.333, Total Loss : 7.095 | 10 iter time 0.3223: \n",
            "Current Batch 30 / 132 | Cls Loss : 2.632, Loc Loss : 4.353, Total Loss : 6.985 | 10 iter time 0.3215: \n",
            "Current Batch 40 / 132 | Cls Loss : 2.159, Loc Loss : 4.366, Total Loss : 6.525 | 10 iter time 0.3266: \n",
            "Current Batch 50 / 132 | Cls Loss : 1.986, Loc Loss : 4.345, Total Loss : 6.331 | 10 iter time 0.3711: \n",
            "Current Batch 60 / 132 | Cls Loss : 2.452, Loc Loss : 4.375, Total Loss : 6.826 | 10 iter time 0.3161: \n",
            "Current Batch 70 / 132 | Cls Loss : 2.333, Loc Loss : 4.379, Total Loss : 6.711 | 10 iter time 0.3204: \n",
            "Current Batch 80 / 132 | Cls Loss : 2.705, Loc Loss : 4.357, Total Loss : 7.062 | 10 iter time 0.3435: \n",
            "Current Batch 90 / 132 | Cls Loss : 1.763, Loc Loss : 4.354, Total Loss : 6.117 | 10 iter time 0.3254: \n",
            "Current Batch 100 / 132 | Cls Loss : 2.311, Loc Loss : 4.391, Total Loss : 6.702 | 10 iter time 0.3276: \n",
            "Current Batch 110 / 132 | Cls Loss : 1.694, Loc Loss : 4.364, Total Loss : 6.058 | 10 iter time 0.3192: \n",
            "Current Batch 120 / 132 | Cls Loss : 2.582, Loc Loss : 4.355, Total Loss : 6.937 | 10 iter time 0.3260: \n",
            "Current Batch 130 / 132 | Cls Loss : 2.058, Loc Loss : 4.368, Total Loss : 6.426 | 10 iter time 0.3199: \n",
            "Epoch : 9 / 10 of Total Loss : Total Loss : 862.193\n",
            "-----------------------------------------------\n",
            "Epoch : 10 / 10\n",
            "Current Batch 0 / 132 | Cls Loss : 1.935, Loc Loss : 4.346, Total Loss : 6.281 | 10 iter time 0.3349: \n",
            "Current Batch 10 / 132 | Cls Loss : 2.043, Loc Loss : 4.372, Total Loss : 6.415 | 10 iter time 0.3253: \n",
            "Current Batch 20 / 132 | Cls Loss : 2.165, Loc Loss : 4.370, Total Loss : 6.535 | 10 iter time 0.3298: \n",
            "Current Batch 30 / 132 | Cls Loss : 1.845, Loc Loss : 4.332, Total Loss : 6.177 | 10 iter time 0.3300: \n",
            "Current Batch 40 / 132 | Cls Loss : 2.294, Loc Loss : 4.369, Total Loss : 6.663 | 10 iter time 0.3192: \n",
            "Current Batch 50 / 132 | Cls Loss : 2.515, Loc Loss : 4.359, Total Loss : 6.874 | 10 iter time 0.3206: \n",
            "Current Batch 60 / 132 | Cls Loss : 2.463, Loc Loss : 4.359, Total Loss : 6.822 | 10 iter time 0.3344: \n",
            "Current Batch 70 / 132 | Cls Loss : 1.845, Loc Loss : 4.349, Total Loss : 6.194 | 10 iter time 0.3391: \n",
            "Current Batch 80 / 132 | Cls Loss : 2.748, Loc Loss : 4.362, Total Loss : 7.111 | 10 iter time 0.3169: \n",
            "Current Batch 90 / 132 | Cls Loss : 1.539, Loc Loss : 4.347, Total Loss : 5.886 | 10 iter time 0.3199: \n",
            "Current Batch 100 / 132 | Cls Loss : 1.973, Loc Loss : 4.377, Total Loss : 6.351 | 10 iter time 0.3254: \n",
            "Current Batch 110 / 132 | Cls Loss : 1.924, Loc Loss : 4.324, Total Loss : 6.248 | 10 iter time 0.3273: \n",
            "Current Batch 120 / 132 | Cls Loss : 2.184, Loc Loss : 4.331, Total Loss : 6.515 | 10 iter time 0.3205: \n",
            "Current Batch 130 / 132 | Cls Loss : 1.936, Loc Loss : 4.352, Total Loss : 6.289 | 10 iter time 0.3222: \n",
            "Epoch : 10 / 10 of Total Loss : Total Loss : 854.708\n",
            "-----------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(4, 5)"
            ]
          },
          "execution_count": 390,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_step(model, train_dataloader, epoch_num = 10)\n",
        "4, 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d241079",
      "metadata": {
        "id": "1d241079"
      },
      "outputs": [],
      "source": [
        "for idx, data in enumerate(train_dataloader):\n",
        "    print(idx)\n",
        "    print(data[1][0][:,-1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecb3d076",
      "metadata": {
        "id": "ecb3d076"
      },
      "outputs": [],
      "source": [
        "len(train_dataloader)\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99bad913",
      "metadata": {
        "id": "99bad913",
        "outputId": "3c109339-6448-4bc2-8450-614fbd8cf1da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0063,  1.4286],\n",
            "        [ 0.3607,  1.4286],\n",
            "        [ 0.3492,  1.4286],\n",
            "        ...,\n",
            "        [ 0.0382,  1.4286],\n",
            "        [ 0.0382,  1.4286],\n",
            "        [ 0.0540,  1.4286]], device='cuda:0')\n",
            "tensor([[     nan,   1.7837],\n",
            "        [ -5.0973,   1.7837],\n",
            "        [ -5.2586,   1.7837],\n",
            "        ...,\n",
            "        [-16.3142,   1.7837],\n",
            "        [-16.3142,   1.7837],\n",
            "        [-14.5852,   1.7837]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(test1)\n",
        "print(test2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e4999d6",
      "metadata": {
        "id": "2e4999d6",
        "outputId": "7fd57cf2-e468-4209-9fff-e8510e85ef67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0114,  1.4286],\n",
            "        [ 0.1660,  1.4286],\n",
            "        [ 0.1608,  1.4286],\n",
            "        ...,\n",
            "        [ 0.0155,  1.4286],\n",
            "        [ 0.0155,  1.4286],\n",
            "        [ 0.0220,  1.4286]], device='cuda:0')\n",
            "tensor([[     nan,   1.7837],\n",
            "        [ -8.9746,   1.7837],\n",
            "        [ -9.1359,   1.7837],\n",
            "        ...,\n",
            "        [-20.7889,   1.7837],\n",
            "        [-20.7889,   1.7837],\n",
            "        [-19.0654,   1.7837]], device='cuda:0')\n",
            "tensor([[     nan,   1.7837],\n",
            "        [ -8.9746,   1.7837],\n",
            "        [ -9.1359,   1.7837],\n",
            "        ...,\n",
            "        [-20.7889,   1.7837],\n",
            "        [-20.7889,   1.7837],\n",
            "        [-19.0654,   1.7837]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(test1)\n",
        "print(test2)\n",
        "#test1 + 1e-4\n",
        "print(torch.log(test1 + 1e-4) / 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d00e6fa",
      "metadata": {
        "id": "2d00e6fa",
        "outputId": "f04c4527-920e-4bfe-bf29-237b48fd137e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(nan)"
            ]
          },
          "execution_count": 357,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.log(torch.tensor(-0.0063))/0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf7e322e",
      "metadata": {
        "id": "cf7e322e",
        "outputId": "fc50713b-54e5-4e55-8db1-b0396da5bb97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5455, 0.0000, 0.5577, 1.0000],\n",
            "        [0.5836, 0.0000, 0.5974, 1.0000],\n",
            "        [0.6268, 0.0000, 0.6414, 1.0000],\n",
            "        [0.6701, 0.0000, 0.6847, 1.0000],\n",
            "        [0.7076, 0.0000, 0.7211, 1.0000],\n",
            "        [0.7498, 0.0000, 0.7619, 1.0000],\n",
            "        [0.7861, 0.0000, 0.8002, 1.0000],\n",
            "        [0.8367, 0.0000, 0.8523, 1.0000],\n",
            "        [0.8729, 0.0000, 0.8848, 1.0000],\n",
            "        [0.9110, 0.0000, 0.9214, 1.0000],\n",
            "        [0.9598, 0.0000, 0.9724, 1.0000],\n",
            "        [0.9986, 0.0000, 0.9980, 1.0000]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(test3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Test 평가 모드**"
      ],
      "metadata": {
        "id": "OsPUsf_OZl8t"
      },
      "id": "OsPUsf_OZl8t"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "3f033b2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "3f033b2a",
        "outputId": "567eadec-1547-4e5e-b967-3b297c1d779d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-45f7b2f0fcf6>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 테스트 데이터 로더 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# 테스트 데이터 로더 설정\n",
        "train_dataloader = DataLoader(test_loader, batch_size=8, shuffle=False)\n",
        "\n",
        "\n",
        "# 학습 반복\n",
        "num_epochs = 10\n",
        "test_interval = 20  # 테스트 간격 설정\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # 훈련 코드...?\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # 테스트 코드 (test_interval마다 실행)\n",
        "        if (i + 1) % test_interval == 0:\n",
        "            model.eval()  # 모델을 평가 모드로 전환\n",
        "\n",
        "            test_loss = 0.0\n",
        "            all_predictions = []  # 모든 예측을 저장할 리스트\n",
        "            all_labels = []  # 모든 실제 레이블을 저장할 리스트\n",
        "\n",
        "            with torch.no_grad():  # 그라디언트 계산 비활성화\n",
        "                for inputs, labels in test_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    test_loss += criterion(outputs, labels).item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    all_predictions.extend(predicted.tolist())\n",
        "                    all_labels.extend(labels.tolist())\n",
        "\n",
        "            test_loss /= len(test_loader)\n",
        "\n",
        "            # 여기에서 mAP 계산 및 출력\n",
        "            # 예측과 레이블을 사용하여 mAP를 계산하는 추가 코드 필요\n",
        "\n",
        "            def calculate_mAP(model, test_loader, num_classes, device):\n",
        "              model.eval()  # 모델을 평가 모드로 설정\n",
        "              all_predictions = []\n",
        "              all_labels = []\n",
        "\n",
        "              with torch.no_grad():\n",
        "                for inputs, labels in test_loader:\n",
        "                  inputs = inputs.to(device)\n",
        "                  outputs = model(inputs)\n",
        "                  _, predicted = torch.max(outputs, 1)\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.numpy())\n",
        "\n",
        "                all_predictions = np.array(all_predictions)\n",
        "                all_labels = np.array(all_labels)\n",
        "\n",
        "    # 각 클래스에 대한 AP 계산\n",
        "                average_precisions = []\n",
        "                for class_id in range(num_classes):\n",
        "                    class_mask = (all_labels == class_id)\n",
        "                    class_predictions = all_predictions[class_mask]\n",
        "                    class_labels = all_labels[class_mask]\n",
        "                    true_positives = (class_predictions == class_labels).astype(np.float32)\n",
        "                    false_positives = (class_predictions != class_labels).astype(np.float32)\n",
        "\n",
        "                    num_true_positives = np.cumsum(true_positives)\n",
        "                    num_false_positives = np.cumsum(false_positives)\n",
        "\n",
        "                    recall = num_true_positives / np.maximum(\n",
        "                        num_true_positives + num_false_positives, np.finfo(np.float64).eps)\n",
        "\n",
        "                    precision = num_true_positives / np.maximum(\n",
        "                        num_true_positives + num_false_positives, np.finfo(np.float64).eps)\n",
        "\n",
        "        # AP 계산\n",
        "                    average_precision = calculate_ap(recall, precision)\n",
        "                    average_precisions.append(average_precision)\n",
        "\n",
        "    # 모든 클래스의 AP의 평균을 계산하여 mAP를 얻음\n",
        "                mAP = np.mean(average_precisions)\n",
        "                return mAP\n",
        "\n",
        "\n",
        "                def calculate_ap(recall, precision):\n",
        "\n",
        "                  recall_thresholds = np.linspace(0, 1, 11)\n",
        "                  precision_sum = 0\n",
        "                  num_recall_thresholds = 0\n",
        "\n",
        "                  for threshold in recall_thresholds:\n",
        "                    mask = recall >= threshold\n",
        "\n",
        "                    if np.any(mask):\n",
        "                      precision_sum += np.max(precision[mask])\n",
        "                      num_recall_thresholds += 1\n",
        "\n",
        "                      average_precision = precision_sum / max(1, num_recall_thresholds) # 위의 calculate_mAP 함수를 사용하여 mAP를 계산\n",
        "                      return average_precision\n",
        "\n",
        "                      num_classes = conf_data.size(2)\n",
        "                      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU 사용 가능한 경우 CUDA로 설정\n",
        "                      mAP = calculate_mAP(model, test_loader, num_classes, device)\n",
        "                      print(f\"mAP: {mAP}\")\n",
        "\n",
        "            print(f'Epoch {epoch + 1}, Iter {i + 1}, Train Loss: {running_loss / test_interval}, Test Loss: {test_loss}')\n",
        "\n",
        "            # 모델을 다시 훈련 모드로 전환\n",
        "            model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.mAP 계산 기능 추가**\n",
        "\n",
        "*   calculate_mAP 함수를 사용하여 모델의 평균 정밀도 (mAP)를 계산하려면 모델의 예측과 실제 라벨을 사용하여 각 이미지에 대한 Average Precision (AP)을 계산하고 이를 평균화해야 함\n",
        "\n",
        "코드 작성 단계\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.  이미지별로 예측과 실제 라벨을 저장할 리스트를 생성합니다.\n",
        "2. 모델의 예측을 테스트 세트 이미지에 대해 수행하고 예측된 박스와 클래스 정보를 얻습니다.\n",
        "3. 각 이미지에 대해 예측된 박스와 실제 라벨을 리스트에 추가합니다.\n",
        "4. 모든 이미지에 대해 AP를 계산하고 이를 모두 저장합니다.\n",
        "5. AP의 평균값을 계산하여 mAP를 얻습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ws23Mg34vsh6"
      },
      "id": "Ws23Mg34vsh6"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "def calculate_mAP(model, test_loader):\n",
        "    # 모델을 평가 모드로 설정\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []  # 모든 이미지의 예측 정보 저장\n",
        "    all_labels = []      # 모든 이미지의 실제 라벨 정보 저장\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in test_loader:\n",
        "            # 이미지를 모델에 전달하여 예측 수행\n",
        "            outputs = model(images)\n",
        "            predictions = post_process(outputs)  # 모델의 출력을 후처리하여 예측된 박스 및 클래스 얻음\n",
        "\n",
        "            # 예측과 실제 라벨을 리스트에 추가\n",
        "            all_predictions.extend(predictions)\n",
        "            all_labels.extend(targets)\n",
        "\n",
        "    # AP를 계산하고 저장할 리스트\n",
        "    ap_scores = []\n",
        "\n",
        "    for i in range(len(all_labels)):\n",
        "        # 각 이미지에 대한 예측과 실제 라벨 가져오기\n",
        "        predictions = all_predictions[i]\n",
        "        labels = all_labels[i]\n",
        "\n",
        "        # AP 계산을 위해 실제 라벨을 이진 벡터로 변환\n",
        "        true_labels = np.zeros(len(predictions))\n",
        "        true_labels[labels] = 1\n",
        "\n",
        "        # sklearn의 average_precision_score를 사용하여 AP 계산\n",
        "        ap = average_precision_score(true_labels, predictions)\n",
        "        ap_scores.append(ap)\n",
        "\n",
        "    # 모든 이미지에 대한 AP의 평균을 계산하여 mAP 얻음\n",
        "    mAP = np.mean(ap_scores)\n",
        "\n",
        "    return mAP\n",
        "\n",
        "# 모델 예측을 후처리하는 함수 (필요에 따라 수정)\n",
        "def post_process(outputs):\n",
        "    # 모델의 출력을 후처리하여 예측된 박스와 클래스를 반환\n",
        "     (outputs, priors, conf_threshold=0.5, nms_threshold =0.5):\n",
        "     confidences, locations = outputs\n",
        "     num_classes = confidences.size(2)\n",
        "\n",
        "    # 바운딩 박스 디코딩\n",
        "    decoded_boxes = decode(locations, priors)\n",
        "\n",
        "    # 각 클래스에 대해 NMS를 수행하고, confidence_threshold 이상인 예측만 유지\n",
        "    all_predictions = []\n",
        "    for class_idx in range(1, num_classes):\n",
        "        class_scores = confidences[:, :, class_idx]  # 해당 클래스의 점수\n",
        "        mask = class_scores > conf_threshold\n",
        "        class_boxes = decoded_boxes[mask]\n",
        "        class_scores = class_scores[mask]\n",
        "\n",
        "        # NMS 적용\n",
        "        keep = nms(class_boxes, class_scores, nms_threshold)\n",
        "        class_boxes = class_boxes[keep]\n",
        "        class_scores = class_scores[keep]\n",
        "\n",
        "        # 예측된 바운딩 박스와 클래스 점수를 리스트에 추가\n",
        "        predictions = torch.cat([class_boxes, class_scores.unsqueeze(1)], dim=1)\n",
        "        all_predictions.append(predictions)\n",
        "\n",
        "    if len(all_predictions) == 0:\n",
        "        return torch.tensor([])\n",
        "\n",
        "    # 모든 클래스의 예측을 하나의 텐서로 연결\n",
        "    all_predictions = torch.cat(all_predictions, dim=0)\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "# 예측 후처리를 수행한 결과\n",
        "predictions = post_process(outputs, priors, conf_threshold=0.5, nms_threshold=0.5)\n",
        "\n",
        "\n",
        "# 모델과 테스트 데이터 로더를 사용하여 mAP 계산\n",
        "mAP = calculate_mAP(model, test_loader)\n",
        "print(f\"Mean Average Precision (mAP): {mAP}\")\n",
        "\n",
        "# conf_threshold와 nms_threshold는 클래스 점수와 NMS의 임계값을 나타냄을 참고"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "310v4t1P0WYV",
        "outputId": "0af2fdc2-625b-442f-a50f-d6035e706165"
      },
      "id": "310v4t1P0WYV",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m51\u001b[0m\n\u001b[0;31m    decoded_boxes = decode(locations, priors)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2-2.mAP 계산 (방법 2)\n",
        "하단 코드로 도전하였고,\n",
        "\n",
        "**-> 상단코드 안돌아가고, 하단은 돌아가네요.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V6DgazCp3NyF"
      },
      "id": "V6DgazCp3NyF"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "39d85dae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39d85dae",
        "outputId": "dc5a5f7d-a9ad-416a-c7d7-ea5dbb7549eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision (mAP): <function calculate_mAP at 0x7947184c6b90>\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "def calculate_mAP(model, test_dataset, device='cuda'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_detections = []\n",
        "        all_ground_truths = []\n",
        "        for i in range(len(test_dataset)):\n",
        "            image, targets = test_dataset[i]\n",
        "            image = image.to(device)\n",
        "            detections = model(image.unsqueeze(0))\n",
        "            detections = detections.squeeze(0)\n",
        "\n",
        "            # Detections를 박스의 좌표와 확률로 분리\n",
        "            boxes = detections[:, :4]\n",
        "            scores = detections[:, 4]\n",
        "\n",
        "            # 정확도 기준으로 내림차순으로 정렬\n",
        "            sorted_indices = scores.argsort(descending=True)\n",
        "            boxes = boxes[sorted_indices]\n",
        "            scores = scores[sorted_indices]\n",
        "\n",
        "            # 실제 정답 정보\n",
        "            true_boxes = targets[:, :4]\n",
        "            true_labels = targets[:, 4]\n",
        "\n",
        "            all_detections.append((boxes.cpu(), scores.cpu()))\n",
        "            all_ground_truths.append((true_boxes.cpu(), true_labels.cpu()))\n",
        "\n",
        "        # mAP 계산\n",
        "        average_precisions = calculate_average_precisions(all_detections, all_ground_truths)\n",
        "\n",
        "        # 모든 클래스에 대한 mAP 평균\n",
        "        mean_ap = np.mean(average_precisions)\n",
        "\n",
        "        return mean_ap\n",
        "\n",
        "def calculate_average_precisions(all_detections, all_ground_truths):\n",
        "    average_precisions = []\n",
        "    for i in range(len(all_detections)):\n",
        "        detections = all_detections[i]\n",
        "        ground_truths = all_ground_truths[i]\n",
        "\n",
        "        if len(ground_truths) == 0:\n",
        "            continue\n",
        "\n",
        "        detected_flags = [False] * len(ground_truths)\n",
        "        num_detected = 0\n",
        "        sorted_indices = np.argsort(-detections[1])\n",
        "\n",
        "        precision_values = []\n",
        "        recall_values = []\n",
        "\n",
        "        for idx in sorted_indices:\n",
        "            box = detections[0][idx]\n",
        "            score = detections[1][idx]\n",
        "\n",
        "            overlaps = calculate_overlaps(box.unsqueeze(0), ground_truths[0])\n",
        "            max_overlap_idx = np.argmax(overlaps)\n",
        "\n",
        "            if overlaps[max_overlap_idx] > 0.5 and not detected_flags[max_overlap_idx]:\n",
        "                detected_flags[max_overlap_idx] = True\n",
        "                num_detected += 1\n",
        "\n",
        "            precision = num_detected / (idx + 1)\n",
        "            recall = num_detected / len(ground_truths)\n",
        "\n",
        "            precision_values.append(precision)\n",
        "            recall_values.append(recall)\n",
        "\n",
        "        average_precisions.append(calculate_area_under_curve(precision_values, recall_values))\n",
        "\n",
        "    return average_precisions\n",
        "\n",
        "def calculate_area_under_curve(x, y):\n",
        "    sorted_indices = np.argsort(x)\n",
        "    sorted_indices = sorted_indices[::-1]\n",
        "    x = np.array(x)[sorted_indices]\n",
        "    y = np.array(y)[sorted_indices]\n",
        "\n",
        "    auc = 0\n",
        "    prev_x = 0\n",
        "    prev_y = 0\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        width = x[i] - prev_x\n",
        "        height = (y[i] + prev_y) / 2\n",
        "        auc += width * height\n",
        "        prev_x = x[i]\n",
        "        prev_y = y[i]\n",
        "\n",
        "    return auc\n",
        "\n",
        "def calculate_overlaps(boxes1, boxes2):\n",
        "    x1 = boxes1[:, 0]\n",
        "    y1 = boxes1[:, 1]\n",
        "    x2 = boxes1[:, 2]\n",
        "    y2 = boxes1[:, 3]\n",
        "\n",
        "    x1g = boxes2[:, 0]\n",
        "    y1g = boxes2[:, 1]\n",
        "    x2g = boxes2[:, 2]\n",
        "    y2g = boxes2[:, 3]\n",
        "\n",
        "    x1 = x1.unsqueeze(1).expand_as(x1g)\n",
        "    y1 = y1.unsqueeze(1).expand_as(y1g)\n",
        "    x2 = x2.unsqueeze(1).expand_as(x2g)\n",
        "    y2 = y2.unsqueeze(1).expand_as(y2g)\n",
        "\n",
        "    x1g = x1g.unsqueeze(0).expand_as(x1)\n",
        "    y1g = y1g.unsqueeze(0).expand_as(y1)\n",
        "    x2g = x2g.unsqueeze(0).expand_as(x2)\n",
        "    y2g = y2g.unsqueeze(0).expand_as(y2)\n",
        "\n",
        "    inter_x1 = torch.max(x1, x1g)\n",
        "    inter_y1 = torch.max(y1, y1g)\n",
        "    inter_x2 = torch.min(x2, x2g)\n",
        "    inter_y2 = torch.min(y2, y2g)\n",
        "\n",
        "    inter_width = torch.clamp(inter_x2 - inter_x1, min=0)\n",
        "    inter_height = torch.clamp(inter_y2 - inter_y1, min=0)\n",
        "\n",
        "    inter_area = inter_width * inter_height\n",
        "    box1_area = (x2 - x1) * (y2 - y1)\n",
        "    box2_area = (x2g - x1g) * (y2g - y1g)\n",
        "\n",
        "    overlap = inter_area / (box1_area + box2_area - inter_area)\n",
        "\n",
        "    return overlap\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Mean Average Precision (mAP): {calculate_mAP}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. 중심 좌표 x,y와 길이 w,h  (4개의 값)**\n",
        "\n",
        "center_size 함수: 입력으로 주어진 바운딩 박스를 중심 좌표와 크기로 변환\n",
        "decode 함수: 예측된 오프셋 정보와 DBox를 사용하여 원래 바운딩 박스를 계산"
      ],
      "metadata": {
        "id": "fu5YleVg3W-e"
      },
      "id": "fu5YleVg3W-e"
    },
    {
      "cell_type": "code",
      "source": [
        "def center_size(boxes):\n",
        "    return torch.cat((boxes[:, 2:] + boxes[:, :2]) / 2, boxes[:, 2:] - boxes[:, :2], 1)\n",
        "\n",
        "def decode(loc, priors, variances):\n",
        "    boxes = torch.cat((\n",
        "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
        "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "    return boxes\n",
        "\n",
        "print(\"중심 좌표 (x, y):\", x, y)\n",
        "print(\"너비 (w):\", w)\n",
        "print(\"높이 (h):\", h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxDUBIwp3jxi",
        "outputId": "ddc85433-c938-463a-e890-7b13c921f13d"
      },
      "id": "AxDUBIwp3jxi",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "중심 좌표 (x, y): tensor([-0.0004, -0.0102, -0.0114,  ...,  0.0000,  0.0000,  0.1464]) tensor([-0.3237, -0.3237, -0.3237,  ...,  0.1500,  0.1500,  0.1500])\n",
            "너비 (w): tensor([0.0533, 0.0730, 0.0754,  ..., 1.0000, 1.0000, 0.7071])\n",
            "높이 (h): tensor([0.7000, 0.7000, 0.7000,  ..., 0.7000, 0.7000, 0.7000])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}